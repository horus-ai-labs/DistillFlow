student_model:
  model_name_or_path: Qwen/Qwen2-7B-Instruct
  flash_attn: fa2
  use_unsloth: false
  output_attentions: false
  enable_liger_kernel: true
  new_special_tokens: "['<think>', '</think>']"
#  finetuning_args:
#    finetuning_type: lora
  deepspeed_config: './deepspeed/zero2.json'

data:
  seed: 42
  text_field: "text"
  train_datasets:
    - path: horus-ai-labs/R1-Dstill-SFT-gsm8k
      template:
        name: sharegpt
  test_size: 0.2
  streaming: false

distill:
  type: fine-tune
  max_seq_length: 512 # P95 tokens in GSK8k dataset are 338.
  resume_from_checkpoint: null
  temperature: 2.0
  alpha: 0.5
  sft_config:
    output_dir: './results'
    num_train_epochs: 1
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 8
    eval_strategy: steps
    eval_steps: 100
    save_steps: 2000
    logging_steps: 1
    learning_rate: 2.0e-5
#    weight_decay: 0.05
#    warmup_ratio: 0.1
#    lr_scheduler_type: 'cosine'
    fp16: False
    bf16: True
    max_grad_norm: 1.0
    group_by_length: False
    remove_unused_columns: false

