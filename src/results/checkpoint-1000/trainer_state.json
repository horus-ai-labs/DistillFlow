{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.010101010101010102,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 1.0101010101010101e-05,
      "grad_norm": 0.13421429693698883,
      "learning_rate": 6.734006734006735e-10,
      "loss": 0.438,
      "step": 1
    },
    {
      "epoch": 2.0202020202020203e-05,
      "grad_norm": 0.12172448635101318,
      "learning_rate": 1.346801346801347e-09,
      "loss": 0.3543,
      "step": 2
    },
    {
      "epoch": 3.0303030303030302e-05,
      "grad_norm": 0.14874637126922607,
      "learning_rate": 2.0202020202020203e-09,
      "loss": 0.7892,
      "step": 3
    },
    {
      "epoch": 4.0404040404040405e-05,
      "grad_norm": 0.1656404584646225,
      "learning_rate": 2.693602693602694e-09,
      "loss": 0.4972,
      "step": 4
    },
    {
      "epoch": 5.0505050505050505e-05,
      "grad_norm": 0.13392697274684906,
      "learning_rate": 3.3670033670033674e-09,
      "loss": 0.5467,
      "step": 5
    },
    {
      "epoch": 6.0606060606060605e-05,
      "grad_norm": 0.15629839897155762,
      "learning_rate": 4.0404040404040405e-09,
      "loss": 0.5987,
      "step": 6
    },
    {
      "epoch": 7.07070707070707e-05,
      "grad_norm": 0.18072491884231567,
      "learning_rate": 4.713804713804714e-09,
      "loss": 0.6049,
      "step": 7
    },
    {
      "epoch": 8.080808080808081e-05,
      "grad_norm": 0.10706614702939987,
      "learning_rate": 5.387205387205388e-09,
      "loss": 0.5207,
      "step": 8
    },
    {
      "epoch": 9.09090909090909e-05,
      "grad_norm": 0.09394727647304535,
      "learning_rate": 6.060606060606061e-09,
      "loss": 0.5974,
      "step": 9
    },
    {
      "epoch": 0.00010101010101010101,
      "grad_norm": 0.0996222198009491,
      "learning_rate": 6.734006734006735e-09,
      "loss": 0.6935,
      "step": 10
    },
    {
      "epoch": 0.00011111111111111112,
      "grad_norm": 0.1582259237766266,
      "learning_rate": 7.407407407407408e-09,
      "loss": 0.6699,
      "step": 11
    },
    {
      "epoch": 0.00012121212121212121,
      "grad_norm": 0.14146071672439575,
      "learning_rate": 8.080808080808081e-09,
      "loss": 0.7221,
      "step": 12
    },
    {
      "epoch": 0.0001313131313131313,
      "grad_norm": 0.15221206843852997,
      "learning_rate": 8.754208754208756e-09,
      "loss": 0.9631,
      "step": 13
    },
    {
      "epoch": 0.0001414141414141414,
      "grad_norm": 0.1784069687128067,
      "learning_rate": 9.427609427609427e-09,
      "loss": 0.7072,
      "step": 14
    },
    {
      "epoch": 0.00015151515151515152,
      "grad_norm": 0.18209610879421234,
      "learning_rate": 1.0101010101010102e-08,
      "loss": 0.5941,
      "step": 15
    },
    {
      "epoch": 0.00016161616161616162,
      "grad_norm": 0.12974265217781067,
      "learning_rate": 1.0774410774410775e-08,
      "loss": 0.4332,
      "step": 16
    },
    {
      "epoch": 0.00017171717171717173,
      "grad_norm": 0.16222456097602844,
      "learning_rate": 1.144781144781145e-08,
      "loss": 0.4793,
      "step": 17
    },
    {
      "epoch": 0.0001818181818181818,
      "grad_norm": 0.1274576038122177,
      "learning_rate": 1.2121212121212122e-08,
      "loss": 0.7077,
      "step": 18
    },
    {
      "epoch": 0.00019191919191919191,
      "grad_norm": 0.1621808409690857,
      "learning_rate": 1.2794612794612795e-08,
      "loss": 0.4871,
      "step": 19
    },
    {
      "epoch": 0.00020202020202020202,
      "grad_norm": 0.13337965309619904,
      "learning_rate": 1.346801346801347e-08,
      "loss": 0.6904,
      "step": 20
    },
    {
      "epoch": 0.00021212121212121213,
      "grad_norm": 0.2128429412841797,
      "learning_rate": 1.4141414141414143e-08,
      "loss": 0.5402,
      "step": 21
    },
    {
      "epoch": 0.00022222222222222223,
      "grad_norm": 0.1268814206123352,
      "learning_rate": 1.4814814814814816e-08,
      "loss": 0.6554,
      "step": 22
    },
    {
      "epoch": 0.0002323232323232323,
      "grad_norm": 0.10819172114133835,
      "learning_rate": 1.548821548821549e-08,
      "loss": 0.5829,
      "step": 23
    },
    {
      "epoch": 0.00024242424242424242,
      "grad_norm": 0.19298827648162842,
      "learning_rate": 1.6161616161616162e-08,
      "loss": 0.7844,
      "step": 24
    },
    {
      "epoch": 0.0002525252525252525,
      "grad_norm": 0.17683479189872742,
      "learning_rate": 1.6835016835016835e-08,
      "loss": 0.9068,
      "step": 25
    },
    {
      "epoch": 0.0002626262626262626,
      "grad_norm": 0.24406202137470245,
      "learning_rate": 1.7508417508417512e-08,
      "loss": 1.148,
      "step": 26
    },
    {
      "epoch": 0.00027272727272727274,
      "grad_norm": 0.14055506885051727,
      "learning_rate": 1.8181818181818185e-08,
      "loss": 0.7414,
      "step": 27
    },
    {
      "epoch": 0.0002828282828282828,
      "grad_norm": 0.17774198949337006,
      "learning_rate": 1.8855218855218855e-08,
      "loss": 0.5325,
      "step": 28
    },
    {
      "epoch": 0.00029292929292929295,
      "grad_norm": 0.14225444197654724,
      "learning_rate": 1.952861952861953e-08,
      "loss": 0.487,
      "step": 29
    },
    {
      "epoch": 0.00030303030303030303,
      "grad_norm": 0.14804908633232117,
      "learning_rate": 2.0202020202020204e-08,
      "loss": 0.6322,
      "step": 30
    },
    {
      "epoch": 0.0003131313131313131,
      "grad_norm": 0.5360762476921082,
      "learning_rate": 2.0875420875420877e-08,
      "loss": 0.8639,
      "step": 31
    },
    {
      "epoch": 0.00032323232323232324,
      "grad_norm": 0.23511265218257904,
      "learning_rate": 2.154882154882155e-08,
      "loss": 1.0801,
      "step": 32
    },
    {
      "epoch": 0.0003333333333333333,
      "grad_norm": 0.10104915499687195,
      "learning_rate": 2.2222222222222224e-08,
      "loss": 0.9853,
      "step": 33
    },
    {
      "epoch": 0.00034343434343434346,
      "grad_norm": 0.1255740523338318,
      "learning_rate": 2.28956228956229e-08,
      "loss": 0.3487,
      "step": 34
    },
    {
      "epoch": 0.00035353535353535354,
      "grad_norm": 0.24050115048885345,
      "learning_rate": 2.356902356902357e-08,
      "loss": 0.9806,
      "step": 35
    },
    {
      "epoch": 0.0003636363636363636,
      "grad_norm": 0.09545484930276871,
      "learning_rate": 2.4242424242424243e-08,
      "loss": 0.7787,
      "step": 36
    },
    {
      "epoch": 0.00037373737373737375,
      "grad_norm": 0.20258082449436188,
      "learning_rate": 2.491582491582492e-08,
      "loss": 0.9,
      "step": 37
    },
    {
      "epoch": 0.00038383838383838383,
      "grad_norm": 0.2597730755805969,
      "learning_rate": 2.558922558922559e-08,
      "loss": 0.9779,
      "step": 38
    },
    {
      "epoch": 0.00039393939393939396,
      "grad_norm": 0.1366550475358963,
      "learning_rate": 2.6262626262626266e-08,
      "loss": 0.3685,
      "step": 39
    },
    {
      "epoch": 0.00040404040404040404,
      "grad_norm": 0.10521057993173599,
      "learning_rate": 2.693602693602694e-08,
      "loss": 0.6044,
      "step": 40
    },
    {
      "epoch": 0.0004141414141414141,
      "grad_norm": 0.14410534501075745,
      "learning_rate": 2.760942760942761e-08,
      "loss": 0.7123,
      "step": 41
    },
    {
      "epoch": 0.00042424242424242425,
      "grad_norm": 0.12549275159835815,
      "learning_rate": 2.8282828282828285e-08,
      "loss": 0.3948,
      "step": 42
    },
    {
      "epoch": 0.00043434343434343433,
      "grad_norm": 0.10938499122858047,
      "learning_rate": 2.895622895622896e-08,
      "loss": 0.5678,
      "step": 43
    },
    {
      "epoch": 0.00044444444444444447,
      "grad_norm": 0.11344257742166519,
      "learning_rate": 2.9629629629629632e-08,
      "loss": 0.5931,
      "step": 44
    },
    {
      "epoch": 0.00045454545454545455,
      "grad_norm": 0.21254561841487885,
      "learning_rate": 3.0303030303030305e-08,
      "loss": 0.7865,
      "step": 45
    },
    {
      "epoch": 0.0004646464646464646,
      "grad_norm": 0.12155268341302872,
      "learning_rate": 3.097643097643098e-08,
      "loss": 0.585,
      "step": 46
    },
    {
      "epoch": 0.00047474747474747476,
      "grad_norm": 0.1804099977016449,
      "learning_rate": 3.164983164983165e-08,
      "loss": 0.8709,
      "step": 47
    },
    {
      "epoch": 0.00048484848484848484,
      "grad_norm": 0.14290833473205566,
      "learning_rate": 3.2323232323232324e-08,
      "loss": 0.8019,
      "step": 48
    },
    {
      "epoch": 0.000494949494949495,
      "grad_norm": 0.1469186395406723,
      "learning_rate": 3.2996632996633e-08,
      "loss": 0.5823,
      "step": 49
    },
    {
      "epoch": 0.000505050505050505,
      "grad_norm": 0.11849448829889297,
      "learning_rate": 3.367003367003367e-08,
      "loss": 0.6644,
      "step": 50
    },
    {
      "epoch": 0.0005151515151515151,
      "grad_norm": 0.16157837212085724,
      "learning_rate": 3.4343434343434344e-08,
      "loss": 0.4968,
      "step": 51
    },
    {
      "epoch": 0.0005252525252525252,
      "grad_norm": 0.12883403897285461,
      "learning_rate": 3.5016835016835024e-08,
      "loss": 0.611,
      "step": 52
    },
    {
      "epoch": 0.0005353535353535354,
      "grad_norm": 0.11410505324602127,
      "learning_rate": 3.569023569023569e-08,
      "loss": 0.5857,
      "step": 53
    },
    {
      "epoch": 0.0005454545454545455,
      "grad_norm": 0.19443060457706451,
      "learning_rate": 3.636363636363637e-08,
      "loss": 0.5809,
      "step": 54
    },
    {
      "epoch": 0.0005555555555555556,
      "grad_norm": 0.10762275010347366,
      "learning_rate": 3.703703703703704e-08,
      "loss": 0.4556,
      "step": 55
    },
    {
      "epoch": 0.0005656565656565656,
      "grad_norm": 0.15840113162994385,
      "learning_rate": 3.771043771043771e-08,
      "loss": 0.5678,
      "step": 56
    },
    {
      "epoch": 0.0005757575757575757,
      "grad_norm": 0.22563089430332184,
      "learning_rate": 3.838383838383839e-08,
      "loss": 0.975,
      "step": 57
    },
    {
      "epoch": 0.0005858585858585859,
      "grad_norm": 0.15529829263687134,
      "learning_rate": 3.905723905723906e-08,
      "loss": 0.6497,
      "step": 58
    },
    {
      "epoch": 0.000595959595959596,
      "grad_norm": 0.14679482579231262,
      "learning_rate": 3.9730639730639736e-08,
      "loss": 0.7679,
      "step": 59
    },
    {
      "epoch": 0.0006060606060606061,
      "grad_norm": 0.2071901112794876,
      "learning_rate": 4.040404040404041e-08,
      "loss": 1.1507,
      "step": 60
    },
    {
      "epoch": 0.0006161616161616161,
      "grad_norm": 0.23253996670246124,
      "learning_rate": 4.107744107744108e-08,
      "loss": 0.4559,
      "step": 61
    },
    {
      "epoch": 0.0006262626262626262,
      "grad_norm": 0.11191965639591217,
      "learning_rate": 4.1750841750841755e-08,
      "loss": 0.6159,
      "step": 62
    },
    {
      "epoch": 0.0006363636363636364,
      "grad_norm": 0.13775108754634857,
      "learning_rate": 4.2424242424242435e-08,
      "loss": 0.9259,
      "step": 63
    },
    {
      "epoch": 0.0006464646464646465,
      "grad_norm": 0.209858700633049,
      "learning_rate": 4.30976430976431e-08,
      "loss": 0.5988,
      "step": 64
    },
    {
      "epoch": 0.0006565656565656566,
      "grad_norm": 0.10129914432764053,
      "learning_rate": 4.3771043771043774e-08,
      "loss": 0.814,
      "step": 65
    },
    {
      "epoch": 0.0006666666666666666,
      "grad_norm": 0.11958552151918411,
      "learning_rate": 4.444444444444445e-08,
      "loss": 0.8851,
      "step": 66
    },
    {
      "epoch": 0.0006767676767676767,
      "grad_norm": 0.14908292889595032,
      "learning_rate": 4.511784511784512e-08,
      "loss": 1.0239,
      "step": 67
    },
    {
      "epoch": 0.0006868686868686869,
      "grad_norm": 0.11894508451223373,
      "learning_rate": 4.57912457912458e-08,
      "loss": 0.9977,
      "step": 68
    },
    {
      "epoch": 0.000696969696969697,
      "grad_norm": 0.3114427924156189,
      "learning_rate": 4.6464646464646474e-08,
      "loss": 0.6947,
      "step": 69
    },
    {
      "epoch": 0.0007070707070707071,
      "grad_norm": 0.15143470466136932,
      "learning_rate": 4.713804713804714e-08,
      "loss": 0.9729,
      "step": 70
    },
    {
      "epoch": 0.0007171717171717171,
      "grad_norm": 0.11532274633646011,
      "learning_rate": 4.7811447811447813e-08,
      "loss": 0.6284,
      "step": 71
    },
    {
      "epoch": 0.0007272727272727272,
      "grad_norm": 0.08838748186826706,
      "learning_rate": 4.8484848484848486e-08,
      "loss": 0.6289,
      "step": 72
    },
    {
      "epoch": 0.0007373737373737374,
      "grad_norm": 0.13412980735301971,
      "learning_rate": 4.9158249158249166e-08,
      "loss": 0.3956,
      "step": 73
    },
    {
      "epoch": 0.0007474747474747475,
      "grad_norm": 0.36985695362091064,
      "learning_rate": 4.983164983164984e-08,
      "loss": 0.7203,
      "step": 74
    },
    {
      "epoch": 0.0007575757575757576,
      "grad_norm": 0.1839209944009781,
      "learning_rate": 5.050505050505051e-08,
      "loss": 0.8662,
      "step": 75
    },
    {
      "epoch": 0.0007676767676767677,
      "grad_norm": 0.14022532105445862,
      "learning_rate": 5.117845117845118e-08,
      "loss": 0.3483,
      "step": 76
    },
    {
      "epoch": 0.0007777777777777777,
      "grad_norm": 0.12006410211324692,
      "learning_rate": 5.185185185185185e-08,
      "loss": 1.027,
      "step": 77
    },
    {
      "epoch": 0.0007878787878787879,
      "grad_norm": 0.2114449143409729,
      "learning_rate": 5.252525252525253e-08,
      "loss": 0.9246,
      "step": 78
    },
    {
      "epoch": 0.000797979797979798,
      "grad_norm": 0.22657006978988647,
      "learning_rate": 5.3198653198653205e-08,
      "loss": 0.5418,
      "step": 79
    },
    {
      "epoch": 0.0008080808080808081,
      "grad_norm": 0.1750769466161728,
      "learning_rate": 5.387205387205388e-08,
      "loss": 0.6926,
      "step": 80
    },
    {
      "epoch": 0.0008181818181818182,
      "grad_norm": 0.2061653584241867,
      "learning_rate": 5.454545454545455e-08,
      "loss": 1.1238,
      "step": 81
    },
    {
      "epoch": 0.0008282828282828282,
      "grad_norm": 0.20769661664962769,
      "learning_rate": 5.521885521885522e-08,
      "loss": 0.692,
      "step": 82
    },
    {
      "epoch": 0.0008383838383838384,
      "grad_norm": 0.13325481116771698,
      "learning_rate": 5.58922558922559e-08,
      "loss": 0.6225,
      "step": 83
    },
    {
      "epoch": 0.0008484848484848485,
      "grad_norm": 0.3490189015865326,
      "learning_rate": 5.656565656565657e-08,
      "loss": 0.7632,
      "step": 84
    },
    {
      "epoch": 0.0008585858585858586,
      "grad_norm": 0.24013036489486694,
      "learning_rate": 5.7239057239057244e-08,
      "loss": 0.9611,
      "step": 85
    },
    {
      "epoch": 0.0008686868686868687,
      "grad_norm": 0.16820663213729858,
      "learning_rate": 5.791245791245792e-08,
      "loss": 0.7056,
      "step": 86
    },
    {
      "epoch": 0.0008787878787878787,
      "grad_norm": 0.14317403733730316,
      "learning_rate": 5.85858585858586e-08,
      "loss": 0.5338,
      "step": 87
    },
    {
      "epoch": 0.0008888888888888889,
      "grad_norm": 0.2156897932291031,
      "learning_rate": 5.9259259259259263e-08,
      "loss": 0.5783,
      "step": 88
    },
    {
      "epoch": 0.000898989898989899,
      "grad_norm": 0.11208570748567581,
      "learning_rate": 5.993265993265994e-08,
      "loss": 0.6045,
      "step": 89
    },
    {
      "epoch": 0.0009090909090909091,
      "grad_norm": 0.3902907371520996,
      "learning_rate": 6.060606060606061e-08,
      "loss": 0.9204,
      "step": 90
    },
    {
      "epoch": 0.0009191919191919192,
      "grad_norm": 0.1177821084856987,
      "learning_rate": 6.127946127946128e-08,
      "loss": 0.426,
      "step": 91
    },
    {
      "epoch": 0.0009292929292929292,
      "grad_norm": 0.1508103460073471,
      "learning_rate": 6.195286195286196e-08,
      "loss": 0.3333,
      "step": 92
    },
    {
      "epoch": 0.0009393939393939394,
      "grad_norm": 0.07409850507974625,
      "learning_rate": 6.262626262626263e-08,
      "loss": 0.2782,
      "step": 93
    },
    {
      "epoch": 0.0009494949494949495,
      "grad_norm": 0.1300504207611084,
      "learning_rate": 6.32996632996633e-08,
      "loss": 0.2599,
      "step": 94
    },
    {
      "epoch": 0.0009595959595959596,
      "grad_norm": 0.11031711846590042,
      "learning_rate": 6.397306397306398e-08,
      "loss": 0.7553,
      "step": 95
    },
    {
      "epoch": 0.0009696969696969697,
      "grad_norm": 0.25806236267089844,
      "learning_rate": 6.464646464646465e-08,
      "loss": 0.8969,
      "step": 96
    },
    {
      "epoch": 0.0009797979797979799,
      "grad_norm": 0.10790038853883743,
      "learning_rate": 6.531986531986532e-08,
      "loss": 0.5666,
      "step": 97
    },
    {
      "epoch": 0.00098989898989899,
      "grad_norm": 0.199578195810318,
      "learning_rate": 6.5993265993266e-08,
      "loss": 0.6141,
      "step": 98
    },
    {
      "epoch": 0.001,
      "grad_norm": 0.15393909811973572,
      "learning_rate": 6.666666666666668e-08,
      "loss": 0.7718,
      "step": 99
    },
    {
      "epoch": 0.00101010101010101,
      "grad_norm": 0.11507359147071838,
      "learning_rate": 6.734006734006734e-08,
      "loss": 0.6064,
      "step": 100
    },
    {
      "epoch": 0.0010202020202020202,
      "grad_norm": 0.11563224345445633,
      "learning_rate": 6.801346801346801e-08,
      "loss": 0.7146,
      "step": 101
    },
    {
      "epoch": 0.0010303030303030303,
      "grad_norm": 0.19555027782917023,
      "learning_rate": 6.868686868686869e-08,
      "loss": 0.6333,
      "step": 102
    },
    {
      "epoch": 0.0010404040404040403,
      "grad_norm": 0.1856038123369217,
      "learning_rate": 6.936026936026937e-08,
      "loss": 0.8272,
      "step": 103
    },
    {
      "epoch": 0.0010505050505050504,
      "grad_norm": 0.13774509727954865,
      "learning_rate": 7.003367003367005e-08,
      "loss": 0.4215,
      "step": 104
    },
    {
      "epoch": 0.0010606060606060607,
      "grad_norm": 0.13025008141994476,
      "learning_rate": 7.070707070707072e-08,
      "loss": 0.7557,
      "step": 105
    },
    {
      "epoch": 0.0010707070707070708,
      "grad_norm": 0.14056171476840973,
      "learning_rate": 7.138047138047138e-08,
      "loss": 0.7224,
      "step": 106
    },
    {
      "epoch": 0.0010808080808080809,
      "grad_norm": 0.09720776975154877,
      "learning_rate": 7.205387205387205e-08,
      "loss": 0.7128,
      "step": 107
    },
    {
      "epoch": 0.001090909090909091,
      "grad_norm": 0.10764195024967194,
      "learning_rate": 7.272727272727274e-08,
      "loss": 0.5651,
      "step": 108
    },
    {
      "epoch": 0.001101010101010101,
      "grad_norm": 0.24962198734283447,
      "learning_rate": 7.340067340067341e-08,
      "loss": 0.5595,
      "step": 109
    },
    {
      "epoch": 0.0011111111111111111,
      "grad_norm": 0.10612901300191879,
      "learning_rate": 7.407407407407409e-08,
      "loss": 0.5621,
      "step": 110
    },
    {
      "epoch": 0.0011212121212121212,
      "grad_norm": 0.35590100288391113,
      "learning_rate": 7.474747474747476e-08,
      "loss": 0.9535,
      "step": 111
    },
    {
      "epoch": 0.0011313131313131313,
      "grad_norm": 0.10682548582553864,
      "learning_rate": 7.542087542087542e-08,
      "loss": 0.6081,
      "step": 112
    },
    {
      "epoch": 0.0011414141414141413,
      "grad_norm": 0.12260816246271133,
      "learning_rate": 7.60942760942761e-08,
      "loss": 0.8429,
      "step": 113
    },
    {
      "epoch": 0.0011515151515151514,
      "grad_norm": 0.2354649305343628,
      "learning_rate": 7.676767676767678e-08,
      "loss": 0.8154,
      "step": 114
    },
    {
      "epoch": 0.0011616161616161617,
      "grad_norm": 0.16256757080554962,
      "learning_rate": 7.744107744107745e-08,
      "loss": 0.89,
      "step": 115
    },
    {
      "epoch": 0.0011717171717171718,
      "grad_norm": 0.11114752292633057,
      "learning_rate": 7.811447811447812e-08,
      "loss": 0.6132,
      "step": 116
    },
    {
      "epoch": 0.0011818181818181819,
      "grad_norm": 0.1480114459991455,
      "learning_rate": 7.87878787878788e-08,
      "loss": 0.5996,
      "step": 117
    },
    {
      "epoch": 0.001191919191919192,
      "grad_norm": 0.11692429333925247,
      "learning_rate": 7.946127946127947e-08,
      "loss": 0.587,
      "step": 118
    },
    {
      "epoch": 0.001202020202020202,
      "grad_norm": 0.13855165243148804,
      "learning_rate": 8.013468013468014e-08,
      "loss": 0.6612,
      "step": 119
    },
    {
      "epoch": 0.0012121212121212121,
      "grad_norm": 0.1073690727353096,
      "learning_rate": 8.080808080808082e-08,
      "loss": 0.5617,
      "step": 120
    },
    {
      "epoch": 0.0012222222222222222,
      "grad_norm": 0.19216927886009216,
      "learning_rate": 8.148148148148148e-08,
      "loss": 1.065,
      "step": 121
    },
    {
      "epoch": 0.0012323232323232323,
      "grad_norm": 0.23787303268909454,
      "learning_rate": 8.215488215488216e-08,
      "loss": 0.6431,
      "step": 122
    },
    {
      "epoch": 0.0012424242424242424,
      "grad_norm": 0.10593077540397644,
      "learning_rate": 8.282828282828284e-08,
      "loss": 0.5445,
      "step": 123
    },
    {
      "epoch": 0.0012525252525252524,
      "grad_norm": 0.11055520176887512,
      "learning_rate": 8.350168350168351e-08,
      "loss": 0.5846,
      "step": 124
    },
    {
      "epoch": 0.0012626262626262627,
      "grad_norm": 0.12694409489631653,
      "learning_rate": 8.417508417508418e-08,
      "loss": 0.3939,
      "step": 125
    },
    {
      "epoch": 0.0012727272727272728,
      "grad_norm": 0.11711924523115158,
      "learning_rate": 8.484848484848487e-08,
      "loss": 0.7112,
      "step": 126
    },
    {
      "epoch": 0.001282828282828283,
      "grad_norm": 0.12265952676534653,
      "learning_rate": 8.552188552188553e-08,
      "loss": 0.8041,
      "step": 127
    },
    {
      "epoch": 0.001292929292929293,
      "grad_norm": 0.19321206212043762,
      "learning_rate": 8.61952861952862e-08,
      "loss": 0.7063,
      "step": 128
    },
    {
      "epoch": 0.001303030303030303,
      "grad_norm": 0.1918192207813263,
      "learning_rate": 8.686868686868688e-08,
      "loss": 0.6105,
      "step": 129
    },
    {
      "epoch": 0.0013131313131313131,
      "grad_norm": 0.13006506860256195,
      "learning_rate": 8.754208754208755e-08,
      "loss": 0.636,
      "step": 130
    },
    {
      "epoch": 0.0013232323232323232,
      "grad_norm": 0.17677365243434906,
      "learning_rate": 8.821548821548824e-08,
      "loss": 0.7504,
      "step": 131
    },
    {
      "epoch": 0.0013333333333333333,
      "grad_norm": 0.11388320475816727,
      "learning_rate": 8.88888888888889e-08,
      "loss": 0.446,
      "step": 132
    },
    {
      "epoch": 0.0013434343434343434,
      "grad_norm": 0.10210270434617996,
      "learning_rate": 8.956228956228957e-08,
      "loss": 0.7084,
      "step": 133
    },
    {
      "epoch": 0.0013535353535353534,
      "grad_norm": 0.10904247313737869,
      "learning_rate": 9.023569023569024e-08,
      "loss": 0.4537,
      "step": 134
    },
    {
      "epoch": 0.0013636363636363637,
      "grad_norm": 0.1406918466091156,
      "learning_rate": 9.090909090909091e-08,
      "loss": 0.5534,
      "step": 135
    },
    {
      "epoch": 0.0013737373737373738,
      "grad_norm": 0.15757906436920166,
      "learning_rate": 9.15824915824916e-08,
      "loss": 0.3778,
      "step": 136
    },
    {
      "epoch": 0.001383838383838384,
      "grad_norm": 0.15861453115940094,
      "learning_rate": 9.225589225589226e-08,
      "loss": 0.8528,
      "step": 137
    },
    {
      "epoch": 0.001393939393939394,
      "grad_norm": 0.19519612193107605,
      "learning_rate": 9.292929292929295e-08,
      "loss": 0.9248,
      "step": 138
    },
    {
      "epoch": 0.001404040404040404,
      "grad_norm": 0.1343485414981842,
      "learning_rate": 9.360269360269361e-08,
      "loss": 0.6001,
      "step": 139
    },
    {
      "epoch": 0.0014141414141414141,
      "grad_norm": 0.15188433229923248,
      "learning_rate": 9.427609427609428e-08,
      "loss": 0.475,
      "step": 140
    },
    {
      "epoch": 0.0014242424242424242,
      "grad_norm": 0.14855605363845825,
      "learning_rate": 9.494949494949497e-08,
      "loss": 0.4193,
      "step": 141
    },
    {
      "epoch": 0.0014343434343434343,
      "grad_norm": 0.19940055906772614,
      "learning_rate": 9.562289562289563e-08,
      "loss": 1.2502,
      "step": 142
    },
    {
      "epoch": 0.0014444444444444444,
      "grad_norm": 0.09788231551647186,
      "learning_rate": 9.629629629629631e-08,
      "loss": 0.679,
      "step": 143
    },
    {
      "epoch": 0.0014545454545454545,
      "grad_norm": 0.08631929755210876,
      "learning_rate": 9.696969696969697e-08,
      "loss": 0.5767,
      "step": 144
    },
    {
      "epoch": 0.0014646464646464648,
      "grad_norm": 0.13474969565868378,
      "learning_rate": 9.764309764309765e-08,
      "loss": 0.5638,
      "step": 145
    },
    {
      "epoch": 0.0014747474747474748,
      "grad_norm": 0.10817217826843262,
      "learning_rate": 9.831649831649833e-08,
      "loss": 0.5822,
      "step": 146
    },
    {
      "epoch": 0.001484848484848485,
      "grad_norm": 0.2886560559272766,
      "learning_rate": 9.898989898989899e-08,
      "loss": 0.9423,
      "step": 147
    },
    {
      "epoch": 0.001494949494949495,
      "grad_norm": 0.1362755447626114,
      "learning_rate": 9.966329966329968e-08,
      "loss": 0.4067,
      "step": 148
    },
    {
      "epoch": 0.001505050505050505,
      "grad_norm": 0.12371993809938431,
      "learning_rate": 1.0033670033670034e-07,
      "loss": 1.0984,
      "step": 149
    },
    {
      "epoch": 0.0015151515151515152,
      "grad_norm": 0.19630877673625946,
      "learning_rate": 1.0101010101010103e-07,
      "loss": 0.7064,
      "step": 150
    },
    {
      "epoch": 0.0015252525252525252,
      "grad_norm": 0.1779646873474121,
      "learning_rate": 1.016835016835017e-07,
      "loss": 0.6328,
      "step": 151
    },
    {
      "epoch": 0.0015353535353535353,
      "grad_norm": 0.1649138331413269,
      "learning_rate": 1.0235690235690236e-07,
      "loss": 0.5369,
      "step": 152
    },
    {
      "epoch": 0.0015454545454545454,
      "grad_norm": 0.11769156157970428,
      "learning_rate": 1.0303030303030304e-07,
      "loss": 0.8367,
      "step": 153
    },
    {
      "epoch": 0.0015555555555555555,
      "grad_norm": 0.15936650335788727,
      "learning_rate": 1.037037037037037e-07,
      "loss": 0.5237,
      "step": 154
    },
    {
      "epoch": 0.0015656565656565658,
      "grad_norm": 0.1034262478351593,
      "learning_rate": 1.0437710437710439e-07,
      "loss": 0.7394,
      "step": 155
    },
    {
      "epoch": 0.0015757575757575758,
      "grad_norm": 0.1103605255484581,
      "learning_rate": 1.0505050505050506e-07,
      "loss": 0.5591,
      "step": 156
    },
    {
      "epoch": 0.001585858585858586,
      "grad_norm": 0.07804450392723083,
      "learning_rate": 1.0572390572390572e-07,
      "loss": 0.3151,
      "step": 157
    },
    {
      "epoch": 0.001595959595959596,
      "grad_norm": 0.10849356651306152,
      "learning_rate": 1.0639730639730641e-07,
      "loss": 0.5842,
      "step": 158
    },
    {
      "epoch": 0.001606060606060606,
      "grad_norm": 0.2211136370897293,
      "learning_rate": 1.0707070707070707e-07,
      "loss": 0.8347,
      "step": 159
    },
    {
      "epoch": 0.0016161616161616162,
      "grad_norm": 0.11386308073997498,
      "learning_rate": 1.0774410774410776e-07,
      "loss": 0.6709,
      "step": 160
    },
    {
      "epoch": 0.0016262626262626262,
      "grad_norm": 0.12031926214694977,
      "learning_rate": 1.0841750841750843e-07,
      "loss": 0.5698,
      "step": 161
    },
    {
      "epoch": 0.0016363636363636363,
      "grad_norm": 0.12515084445476532,
      "learning_rate": 1.090909090909091e-07,
      "loss": 0.5681,
      "step": 162
    },
    {
      "epoch": 0.0016464646464646464,
      "grad_norm": 0.19132298231124878,
      "learning_rate": 1.0976430976430978e-07,
      "loss": 1.0034,
      "step": 163
    },
    {
      "epoch": 0.0016565656565656565,
      "grad_norm": 0.15622162818908691,
      "learning_rate": 1.1043771043771044e-07,
      "loss": 0.5264,
      "step": 164
    },
    {
      "epoch": 0.0016666666666666668,
      "grad_norm": 0.17179183661937714,
      "learning_rate": 1.1111111111111112e-07,
      "loss": 0.6176,
      "step": 165
    },
    {
      "epoch": 0.0016767676767676769,
      "grad_norm": 0.14681756496429443,
      "learning_rate": 1.117845117845118e-07,
      "loss": 0.8749,
      "step": 166
    },
    {
      "epoch": 0.001686868686868687,
      "grad_norm": 0.10514775663614273,
      "learning_rate": 1.1245791245791247e-07,
      "loss": 0.7134,
      "step": 167
    },
    {
      "epoch": 0.001696969696969697,
      "grad_norm": 0.12646588683128357,
      "learning_rate": 1.1313131313131314e-07,
      "loss": 0.4962,
      "step": 168
    },
    {
      "epoch": 0.001707070707070707,
      "grad_norm": 0.11781284213066101,
      "learning_rate": 1.138047138047138e-07,
      "loss": 0.664,
      "step": 169
    },
    {
      "epoch": 0.0017171717171717172,
      "grad_norm": 0.16220593452453613,
      "learning_rate": 1.1447811447811449e-07,
      "loss": 0.5752,
      "step": 170
    },
    {
      "epoch": 0.0017272727272727272,
      "grad_norm": 0.21798180043697357,
      "learning_rate": 1.1515151515151516e-07,
      "loss": 0.8559,
      "step": 171
    },
    {
      "epoch": 0.0017373737373737373,
      "grad_norm": 0.2165617048740387,
      "learning_rate": 1.1582491582491583e-07,
      "loss": 1.2326,
      "step": 172
    },
    {
      "epoch": 0.0017474747474747474,
      "grad_norm": 0.16491861641407013,
      "learning_rate": 1.1649831649831651e-07,
      "loss": 0.7047,
      "step": 173
    },
    {
      "epoch": 0.0017575757575757575,
      "grad_norm": 0.21587732434272766,
      "learning_rate": 1.171717171717172e-07,
      "loss": 0.5502,
      "step": 174
    },
    {
      "epoch": 0.0017676767676767678,
      "grad_norm": 0.12706267833709717,
      "learning_rate": 1.1784511784511785e-07,
      "loss": 0.3842,
      "step": 175
    },
    {
      "epoch": 0.0017777777777777779,
      "grad_norm": 0.09346020966768265,
      "learning_rate": 1.1851851851851853e-07,
      "loss": 0.6387,
      "step": 176
    },
    {
      "epoch": 0.001787878787878788,
      "grad_norm": 0.0919828936457634,
      "learning_rate": 1.191919191919192e-07,
      "loss": 0.5462,
      "step": 177
    },
    {
      "epoch": 0.001797979797979798,
      "grad_norm": 0.1497562974691391,
      "learning_rate": 1.1986531986531987e-07,
      "loss": 0.6717,
      "step": 178
    },
    {
      "epoch": 0.001808080808080808,
      "grad_norm": 0.2072773575782776,
      "learning_rate": 1.2053872053872056e-07,
      "loss": 0.8672,
      "step": 179
    },
    {
      "epoch": 0.0018181818181818182,
      "grad_norm": 0.10047515481710434,
      "learning_rate": 1.2121212121212122e-07,
      "loss": 0.7081,
      "step": 180
    },
    {
      "epoch": 0.0018282828282828283,
      "grad_norm": 0.34529998898506165,
      "learning_rate": 1.2188552188552188e-07,
      "loss": 0.9393,
      "step": 181
    },
    {
      "epoch": 0.0018383838383838383,
      "grad_norm": 0.11382832378149033,
      "learning_rate": 1.2255892255892257e-07,
      "loss": 0.859,
      "step": 182
    },
    {
      "epoch": 0.0018484848484848484,
      "grad_norm": 0.10749665647745132,
      "learning_rate": 1.2323232323232323e-07,
      "loss": 0.6114,
      "step": 183
    },
    {
      "epoch": 0.0018585858585858585,
      "grad_norm": 0.18212859332561493,
      "learning_rate": 1.239057239057239e-07,
      "loss": 0.4821,
      "step": 184
    },
    {
      "epoch": 0.0018686868686868688,
      "grad_norm": 0.1429215371608734,
      "learning_rate": 1.245791245791246e-07,
      "loss": 0.5784,
      "step": 185
    },
    {
      "epoch": 0.0018787878787878789,
      "grad_norm": 0.26812392473220825,
      "learning_rate": 1.2525252525252526e-07,
      "loss": 1.3343,
      "step": 186
    },
    {
      "epoch": 0.001888888888888889,
      "grad_norm": 0.14157606661319733,
      "learning_rate": 1.2592592592592594e-07,
      "loss": 0.4127,
      "step": 187
    },
    {
      "epoch": 0.001898989898989899,
      "grad_norm": 0.1627202033996582,
      "learning_rate": 1.265993265993266e-07,
      "loss": 1.01,
      "step": 188
    },
    {
      "epoch": 0.0019090909090909091,
      "grad_norm": 0.11479641497135162,
      "learning_rate": 1.272727272727273e-07,
      "loss": 0.7877,
      "step": 189
    },
    {
      "epoch": 0.0019191919191919192,
      "grad_norm": 0.116245798766613,
      "learning_rate": 1.2794612794612795e-07,
      "loss": 1.0155,
      "step": 190
    },
    {
      "epoch": 0.0019292929292929293,
      "grad_norm": 0.10310792177915573,
      "learning_rate": 1.2861952861952864e-07,
      "loss": 0.5477,
      "step": 191
    },
    {
      "epoch": 0.0019393939393939393,
      "grad_norm": 0.480437695980072,
      "learning_rate": 1.292929292929293e-07,
      "loss": 0.9094,
      "step": 192
    },
    {
      "epoch": 0.0019494949494949494,
      "grad_norm": 0.2636687755584717,
      "learning_rate": 1.2996632996632996e-07,
      "loss": 1.1492,
      "step": 193
    },
    {
      "epoch": 0.0019595959595959597,
      "grad_norm": 0.14465981721878052,
      "learning_rate": 1.3063973063973064e-07,
      "loss": 0.5132,
      "step": 194
    },
    {
      "epoch": 0.00196969696969697,
      "grad_norm": 0.10962255299091339,
      "learning_rate": 1.3131313131313133e-07,
      "loss": 0.8276,
      "step": 195
    },
    {
      "epoch": 0.00197979797979798,
      "grad_norm": 0.14795976877212524,
      "learning_rate": 1.31986531986532e-07,
      "loss": 0.6441,
      "step": 196
    },
    {
      "epoch": 0.00198989898989899,
      "grad_norm": 0.1089046522974968,
      "learning_rate": 1.3265993265993268e-07,
      "loss": 0.7016,
      "step": 197
    },
    {
      "epoch": 0.002,
      "grad_norm": 0.1809127926826477,
      "learning_rate": 1.3333333333333336e-07,
      "loss": 0.994,
      "step": 198
    },
    {
      "epoch": 0.00201010101010101,
      "grad_norm": 0.13549073040485382,
      "learning_rate": 1.3400673400673402e-07,
      "loss": 0.3249,
      "step": 199
    },
    {
      "epoch": 0.00202020202020202,
      "grad_norm": 0.23868784308433533,
      "learning_rate": 1.3468013468013468e-07,
      "loss": 0.7715,
      "step": 200
    },
    {
      "epoch": 0.0020303030303030303,
      "grad_norm": 0.15265057981014252,
      "learning_rate": 1.3535353535353537e-07,
      "loss": 0.5476,
      "step": 201
    },
    {
      "epoch": 0.0020404040404040404,
      "grad_norm": 0.2026842087507248,
      "learning_rate": 1.3602693602693603e-07,
      "loss": 0.824,
      "step": 202
    },
    {
      "epoch": 0.0020505050505050504,
      "grad_norm": 0.16239644587039948,
      "learning_rate": 1.3670033670033672e-07,
      "loss": 0.5338,
      "step": 203
    },
    {
      "epoch": 0.0020606060606060605,
      "grad_norm": 0.1761707365512848,
      "learning_rate": 1.3737373737373738e-07,
      "loss": 0.4061,
      "step": 204
    },
    {
      "epoch": 0.0020707070707070706,
      "grad_norm": 0.20577670633792877,
      "learning_rate": 1.3804713804713806e-07,
      "loss": 1.0242,
      "step": 205
    },
    {
      "epoch": 0.0020808080808080807,
      "grad_norm": 0.30959317088127136,
      "learning_rate": 1.3872053872053875e-07,
      "loss": 0.7523,
      "step": 206
    },
    {
      "epoch": 0.0020909090909090908,
      "grad_norm": 0.2409992516040802,
      "learning_rate": 1.393939393939394e-07,
      "loss": 0.9799,
      "step": 207
    },
    {
      "epoch": 0.002101010101010101,
      "grad_norm": 0.11584100127220154,
      "learning_rate": 1.400673400673401e-07,
      "loss": 0.685,
      "step": 208
    },
    {
      "epoch": 0.002111111111111111,
      "grad_norm": 0.1694168597459793,
      "learning_rate": 1.4074074074074075e-07,
      "loss": 0.4357,
      "step": 209
    },
    {
      "epoch": 0.0021212121212121214,
      "grad_norm": 0.22856269776821136,
      "learning_rate": 1.4141414141414144e-07,
      "loss": 1.0157,
      "step": 210
    },
    {
      "epoch": 0.0021313131313131315,
      "grad_norm": 0.13332952558994293,
      "learning_rate": 1.420875420875421e-07,
      "loss": 0.6246,
      "step": 211
    },
    {
      "epoch": 0.0021414141414141416,
      "grad_norm": 0.20412135124206543,
      "learning_rate": 1.4276094276094276e-07,
      "loss": 1.0723,
      "step": 212
    },
    {
      "epoch": 0.0021515151515151517,
      "grad_norm": 0.09949760884046555,
      "learning_rate": 1.4343434343434345e-07,
      "loss": 0.6159,
      "step": 213
    },
    {
      "epoch": 0.0021616161616161617,
      "grad_norm": 0.23890471458435059,
      "learning_rate": 1.441077441077441e-07,
      "loss": 0.5414,
      "step": 214
    },
    {
      "epoch": 0.002171717171717172,
      "grad_norm": 0.19397759437561035,
      "learning_rate": 1.447811447811448e-07,
      "loss": 0.5709,
      "step": 215
    },
    {
      "epoch": 0.002181818181818182,
      "grad_norm": 0.18166938424110413,
      "learning_rate": 1.4545454545454548e-07,
      "loss": 0.8589,
      "step": 216
    },
    {
      "epoch": 0.002191919191919192,
      "grad_norm": 0.15009765326976776,
      "learning_rate": 1.4612794612794614e-07,
      "loss": 0.7603,
      "step": 217
    },
    {
      "epoch": 0.002202020202020202,
      "grad_norm": 0.3754429519176483,
      "learning_rate": 1.4680134680134683e-07,
      "loss": 0.7444,
      "step": 218
    },
    {
      "epoch": 0.002212121212121212,
      "grad_norm": 0.1074254959821701,
      "learning_rate": 1.4747474747474749e-07,
      "loss": 0.6409,
      "step": 219
    },
    {
      "epoch": 0.0022222222222222222,
      "grad_norm": 0.09187981486320496,
      "learning_rate": 1.4814814814814817e-07,
      "loss": 0.4606,
      "step": 220
    },
    {
      "epoch": 0.0022323232323232323,
      "grad_norm": 0.22731555998325348,
      "learning_rate": 1.4882154882154883e-07,
      "loss": 0.61,
      "step": 221
    },
    {
      "epoch": 0.0022424242424242424,
      "grad_norm": 0.28786173462867737,
      "learning_rate": 1.4949494949494952e-07,
      "loss": 0.8165,
      "step": 222
    },
    {
      "epoch": 0.0022525252525252525,
      "grad_norm": 0.18643401563167572,
      "learning_rate": 1.5016835016835018e-07,
      "loss": 1.1129,
      "step": 223
    },
    {
      "epoch": 0.0022626262626262625,
      "grad_norm": 0.08183007687330246,
      "learning_rate": 1.5084175084175084e-07,
      "loss": 0.4229,
      "step": 224
    },
    {
      "epoch": 0.0022727272727272726,
      "grad_norm": 0.1440700739622116,
      "learning_rate": 1.5151515151515152e-07,
      "loss": 0.5826,
      "step": 225
    },
    {
      "epoch": 0.0022828282828282827,
      "grad_norm": 0.10239400714635849,
      "learning_rate": 1.521885521885522e-07,
      "loss": 0.896,
      "step": 226
    },
    {
      "epoch": 0.0022929292929292928,
      "grad_norm": 0.10724084079265594,
      "learning_rate": 1.5286195286195287e-07,
      "loss": 0.6044,
      "step": 227
    },
    {
      "epoch": 0.002303030303030303,
      "grad_norm": 0.15811869502067566,
      "learning_rate": 1.5353535353535356e-07,
      "loss": 0.638,
      "step": 228
    },
    {
      "epoch": 0.002313131313131313,
      "grad_norm": 0.11694007366895676,
      "learning_rate": 1.5420875420875422e-07,
      "loss": 0.533,
      "step": 229
    },
    {
      "epoch": 0.0023232323232323234,
      "grad_norm": 0.17729875445365906,
      "learning_rate": 1.548821548821549e-07,
      "loss": 0.3369,
      "step": 230
    },
    {
      "epoch": 0.0023333333333333335,
      "grad_norm": 0.10941772907972336,
      "learning_rate": 1.5555555555555556e-07,
      "loss": 0.5736,
      "step": 231
    },
    {
      "epoch": 0.0023434343434343436,
      "grad_norm": 0.11180593818426132,
      "learning_rate": 1.5622895622895625e-07,
      "loss": 0.5773,
      "step": 232
    },
    {
      "epoch": 0.0023535353535353537,
      "grad_norm": 0.1854778379201889,
      "learning_rate": 1.5690235690235694e-07,
      "loss": 0.818,
      "step": 233
    },
    {
      "epoch": 0.0023636363636363638,
      "grad_norm": 0.14719276130199432,
      "learning_rate": 1.575757575757576e-07,
      "loss": 0.729,
      "step": 234
    },
    {
      "epoch": 0.002373737373737374,
      "grad_norm": 0.15577876567840576,
      "learning_rate": 1.5824915824915826e-07,
      "loss": 0.6251,
      "step": 235
    },
    {
      "epoch": 0.002383838383838384,
      "grad_norm": 0.1422811597585678,
      "learning_rate": 1.5892255892255894e-07,
      "loss": 0.6764,
      "step": 236
    },
    {
      "epoch": 0.002393939393939394,
      "grad_norm": 0.15477396547794342,
      "learning_rate": 1.595959595959596e-07,
      "loss": 0.6835,
      "step": 237
    },
    {
      "epoch": 0.002404040404040404,
      "grad_norm": 0.17365987598896027,
      "learning_rate": 1.602693602693603e-07,
      "loss": 0.679,
      "step": 238
    },
    {
      "epoch": 0.002414141414141414,
      "grad_norm": 0.1516382098197937,
      "learning_rate": 1.6094276094276097e-07,
      "loss": 0.6832,
      "step": 239
    },
    {
      "epoch": 0.0024242424242424242,
      "grad_norm": 0.1861322671175003,
      "learning_rate": 1.6161616161616163e-07,
      "loss": 0.5905,
      "step": 240
    },
    {
      "epoch": 0.0024343434343434343,
      "grad_norm": 0.14600585401058197,
      "learning_rate": 1.622895622895623e-07,
      "loss": 0.5279,
      "step": 241
    },
    {
      "epoch": 0.0024444444444444444,
      "grad_norm": 0.14396701753139496,
      "learning_rate": 1.6296296296296295e-07,
      "loss": 0.6042,
      "step": 242
    },
    {
      "epoch": 0.0024545454545454545,
      "grad_norm": 0.10704442113637924,
      "learning_rate": 1.6363636363636367e-07,
      "loss": 0.2984,
      "step": 243
    },
    {
      "epoch": 0.0024646464646464646,
      "grad_norm": 0.1497144252061844,
      "learning_rate": 1.6430976430976433e-07,
      "loss": 0.5558,
      "step": 244
    },
    {
      "epoch": 0.0024747474747474746,
      "grad_norm": 0.15964201092720032,
      "learning_rate": 1.64983164983165e-07,
      "loss": 0.895,
      "step": 245
    },
    {
      "epoch": 0.0024848484848484847,
      "grad_norm": 0.09908752888441086,
      "learning_rate": 1.6565656565656567e-07,
      "loss": 0.3094,
      "step": 246
    },
    {
      "epoch": 0.002494949494949495,
      "grad_norm": 0.17359043657779694,
      "learning_rate": 1.6632996632996633e-07,
      "loss": 0.7506,
      "step": 247
    },
    {
      "epoch": 0.002505050505050505,
      "grad_norm": 0.1708310842514038,
      "learning_rate": 1.6700336700336702e-07,
      "loss": 0.737,
      "step": 248
    },
    {
      "epoch": 0.002515151515151515,
      "grad_norm": 0.1598178595304489,
      "learning_rate": 1.676767676767677e-07,
      "loss": 0.3304,
      "step": 249
    },
    {
      "epoch": 0.0025252525252525255,
      "grad_norm": 0.13175469636917114,
      "learning_rate": 1.6835016835016837e-07,
      "loss": 0.5565,
      "step": 250
    },
    {
      "epoch": 0.0025353535353535355,
      "grad_norm": 0.1564517468214035,
      "learning_rate": 1.6902356902356903e-07,
      "loss": 0.9227,
      "step": 251
    },
    {
      "epoch": 0.0025454545454545456,
      "grad_norm": 0.16945068538188934,
      "learning_rate": 1.6969696969696974e-07,
      "loss": 0.4785,
      "step": 252
    },
    {
      "epoch": 0.0025555555555555557,
      "grad_norm": 0.17959168553352356,
      "learning_rate": 1.703703703703704e-07,
      "loss": 0.9961,
      "step": 253
    },
    {
      "epoch": 0.002565656565656566,
      "grad_norm": 0.14429397881031036,
      "learning_rate": 1.7104377104377106e-07,
      "loss": 0.7682,
      "step": 254
    },
    {
      "epoch": 0.002575757575757576,
      "grad_norm": 0.16517901420593262,
      "learning_rate": 1.7171717171717172e-07,
      "loss": 0.6952,
      "step": 255
    },
    {
      "epoch": 0.002585858585858586,
      "grad_norm": 0.1429227888584137,
      "learning_rate": 1.723905723905724e-07,
      "loss": 0.5424,
      "step": 256
    },
    {
      "epoch": 0.002595959595959596,
      "grad_norm": 0.09035184234380722,
      "learning_rate": 1.730639730639731e-07,
      "loss": 0.6781,
      "step": 257
    },
    {
      "epoch": 0.002606060606060606,
      "grad_norm": 0.10916712135076523,
      "learning_rate": 1.7373737373737375e-07,
      "loss": 0.8474,
      "step": 258
    },
    {
      "epoch": 0.002616161616161616,
      "grad_norm": 0.17071139812469482,
      "learning_rate": 1.7441077441077444e-07,
      "loss": 0.7475,
      "step": 259
    },
    {
      "epoch": 0.0026262626262626263,
      "grad_norm": 0.12994930148124695,
      "learning_rate": 1.750841750841751e-07,
      "loss": 0.3085,
      "step": 260
    },
    {
      "epoch": 0.0026363636363636363,
      "grad_norm": 0.1123160645365715,
      "learning_rate": 1.7575757575757576e-07,
      "loss": 0.9872,
      "step": 261
    },
    {
      "epoch": 0.0026464646464646464,
      "grad_norm": 0.24465927481651306,
      "learning_rate": 1.7643097643097647e-07,
      "loss": 0.6001,
      "step": 262
    },
    {
      "epoch": 0.0026565656565656565,
      "grad_norm": 0.2028912752866745,
      "learning_rate": 1.7710437710437713e-07,
      "loss": 0.9174,
      "step": 263
    },
    {
      "epoch": 0.0026666666666666666,
      "grad_norm": 0.10659117996692657,
      "learning_rate": 1.777777777777778e-07,
      "loss": 0.5623,
      "step": 264
    },
    {
      "epoch": 0.0026767676767676767,
      "grad_norm": 0.17677120864391327,
      "learning_rate": 1.7845117845117845e-07,
      "loss": 0.8032,
      "step": 265
    },
    {
      "epoch": 0.0026868686868686867,
      "grad_norm": 0.1880558580160141,
      "learning_rate": 1.7912457912457914e-07,
      "loss": 0.504,
      "step": 266
    },
    {
      "epoch": 0.002696969696969697,
      "grad_norm": 0.23049190640449524,
      "learning_rate": 1.7979797979797982e-07,
      "loss": 1.0022,
      "step": 267
    },
    {
      "epoch": 0.002707070707070707,
      "grad_norm": 0.1618092954158783,
      "learning_rate": 1.8047138047138048e-07,
      "loss": 0.4322,
      "step": 268
    },
    {
      "epoch": 0.002717171717171717,
      "grad_norm": 0.13741865754127502,
      "learning_rate": 1.8114478114478117e-07,
      "loss": 0.6742,
      "step": 269
    },
    {
      "epoch": 0.0027272727272727275,
      "grad_norm": 0.1829790621995926,
      "learning_rate": 1.8181818181818183e-07,
      "loss": 0.8489,
      "step": 270
    },
    {
      "epoch": 0.0027373737373737376,
      "grad_norm": 0.15698859095573425,
      "learning_rate": 1.824915824915825e-07,
      "loss": 0.4836,
      "step": 271
    },
    {
      "epoch": 0.0027474747474747476,
      "grad_norm": 0.1546531319618225,
      "learning_rate": 1.831649831649832e-07,
      "loss": 0.5591,
      "step": 272
    },
    {
      "epoch": 0.0027575757575757577,
      "grad_norm": 0.1308411955833435,
      "learning_rate": 1.8383838383838386e-07,
      "loss": 0.5575,
      "step": 273
    },
    {
      "epoch": 0.002767676767676768,
      "grad_norm": 0.10580473393201828,
      "learning_rate": 1.8451178451178452e-07,
      "loss": 0.7813,
      "step": 274
    },
    {
      "epoch": 0.002777777777777778,
      "grad_norm": 0.33695390820503235,
      "learning_rate": 1.8518518518518518e-07,
      "loss": 0.7944,
      "step": 275
    },
    {
      "epoch": 0.002787878787878788,
      "grad_norm": 0.1478404849767685,
      "learning_rate": 1.858585858585859e-07,
      "loss": 1.0785,
      "step": 276
    },
    {
      "epoch": 0.002797979797979798,
      "grad_norm": 0.1376616507768631,
      "learning_rate": 1.8653198653198655e-07,
      "loss": 1.6014,
      "step": 277
    },
    {
      "epoch": 0.002808080808080808,
      "grad_norm": 0.10705385357141495,
      "learning_rate": 1.8720538720538721e-07,
      "loss": 0.5511,
      "step": 278
    },
    {
      "epoch": 0.002818181818181818,
      "grad_norm": 0.41063717007637024,
      "learning_rate": 1.878787878787879e-07,
      "loss": 0.861,
      "step": 279
    },
    {
      "epoch": 0.0028282828282828283,
      "grad_norm": 0.24710071086883545,
      "learning_rate": 1.8855218855218856e-07,
      "loss": 0.6599,
      "step": 280
    },
    {
      "epoch": 0.0028383838383838384,
      "grad_norm": 0.23611287772655487,
      "learning_rate": 1.8922558922558925e-07,
      "loss": 0.5088,
      "step": 281
    },
    {
      "epoch": 0.0028484848484848484,
      "grad_norm": 0.14025536179542542,
      "learning_rate": 1.8989898989898993e-07,
      "loss": 0.7403,
      "step": 282
    },
    {
      "epoch": 0.0028585858585858585,
      "grad_norm": 0.20082394778728485,
      "learning_rate": 1.905723905723906e-07,
      "loss": 0.5688,
      "step": 283
    },
    {
      "epoch": 0.0028686868686868686,
      "grad_norm": 0.1316516250371933,
      "learning_rate": 1.9124579124579125e-07,
      "loss": 0.4341,
      "step": 284
    },
    {
      "epoch": 0.0028787878787878787,
      "grad_norm": 0.13608287274837494,
      "learning_rate": 1.919191919191919e-07,
      "loss": 0.6931,
      "step": 285
    },
    {
      "epoch": 0.0028888888888888888,
      "grad_norm": 0.11342428624629974,
      "learning_rate": 1.9259259259259263e-07,
      "loss": 0.8814,
      "step": 286
    },
    {
      "epoch": 0.002898989898989899,
      "grad_norm": 0.13385608792304993,
      "learning_rate": 1.9326599326599329e-07,
      "loss": 0.667,
      "step": 287
    },
    {
      "epoch": 0.002909090909090909,
      "grad_norm": 0.18041391670703888,
      "learning_rate": 1.9393939393939395e-07,
      "loss": 0.8722,
      "step": 288
    },
    {
      "epoch": 0.002919191919191919,
      "grad_norm": 0.265287309885025,
      "learning_rate": 1.9461279461279463e-07,
      "loss": 0.565,
      "step": 289
    },
    {
      "epoch": 0.0029292929292929295,
      "grad_norm": 0.1667890101671219,
      "learning_rate": 1.952861952861953e-07,
      "loss": 0.6237,
      "step": 290
    },
    {
      "epoch": 0.0029393939393939396,
      "grad_norm": 0.19345469772815704,
      "learning_rate": 1.9595959595959598e-07,
      "loss": 0.8395,
      "step": 291
    },
    {
      "epoch": 0.0029494949494949497,
      "grad_norm": 0.17415665090084076,
      "learning_rate": 1.9663299663299666e-07,
      "loss": 0.3734,
      "step": 292
    },
    {
      "epoch": 0.0029595959595959597,
      "grad_norm": 0.10590758919715881,
      "learning_rate": 1.9730639730639732e-07,
      "loss": 0.6214,
      "step": 293
    },
    {
      "epoch": 0.00296969696969697,
      "grad_norm": 0.13782459497451782,
      "learning_rate": 1.9797979797979798e-07,
      "loss": 0.6435,
      "step": 294
    },
    {
      "epoch": 0.00297979797979798,
      "grad_norm": 0.1472082883119583,
      "learning_rate": 1.9865319865319864e-07,
      "loss": 0.5853,
      "step": 295
    },
    {
      "epoch": 0.00298989898989899,
      "grad_norm": 0.1576939970254898,
      "learning_rate": 1.9932659932659936e-07,
      "loss": 0.3915,
      "step": 296
    },
    {
      "epoch": 0.003,
      "grad_norm": 0.13095630705356598,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 0.9156,
      "step": 297
    },
    {
      "epoch": 0.00301010101010101,
      "grad_norm": 0.17060570418834686,
      "learning_rate": 2.0067340067340068e-07,
      "loss": 1.1098,
      "step": 298
    },
    {
      "epoch": 0.0030202020202020202,
      "grad_norm": 0.1282978504896164,
      "learning_rate": 2.0134680134680136e-07,
      "loss": 0.4915,
      "step": 299
    },
    {
      "epoch": 0.0030303030303030303,
      "grad_norm": 0.3816726505756378,
      "learning_rate": 2.0202020202020205e-07,
      "loss": 0.889,
      "step": 300
    },
    {
      "epoch": 0.0030404040404040404,
      "grad_norm": 0.16812358796596527,
      "learning_rate": 2.026936026936027e-07,
      "loss": 0.6163,
      "step": 301
    },
    {
      "epoch": 0.0030505050505050505,
      "grad_norm": 0.1872316598892212,
      "learning_rate": 2.033670033670034e-07,
      "loss": 0.407,
      "step": 302
    },
    {
      "epoch": 0.0030606060606060605,
      "grad_norm": 0.16259464621543884,
      "learning_rate": 2.0404040404040406e-07,
      "loss": 0.4127,
      "step": 303
    },
    {
      "epoch": 0.0030707070707070706,
      "grad_norm": 0.10573063790798187,
      "learning_rate": 2.0471380471380472e-07,
      "loss": 0.5608,
      "step": 304
    },
    {
      "epoch": 0.0030808080808080807,
      "grad_norm": 0.2467867136001587,
      "learning_rate": 2.0538720538720543e-07,
      "loss": 0.8443,
      "step": 305
    },
    {
      "epoch": 0.0030909090909090908,
      "grad_norm": 0.10586412250995636,
      "learning_rate": 2.060606060606061e-07,
      "loss": 0.5734,
      "step": 306
    },
    {
      "epoch": 0.003101010101010101,
      "grad_norm": 0.1660279780626297,
      "learning_rate": 2.0673400673400675e-07,
      "loss": 0.8326,
      "step": 307
    },
    {
      "epoch": 0.003111111111111111,
      "grad_norm": 0.13061727583408356,
      "learning_rate": 2.074074074074074e-07,
      "loss": 1.4085,
      "step": 308
    },
    {
      "epoch": 0.003121212121212121,
      "grad_norm": 0.10432662814855576,
      "learning_rate": 2.080808080808081e-07,
      "loss": 0.4318,
      "step": 309
    },
    {
      "epoch": 0.0031313131313131315,
      "grad_norm": 0.17630933225154877,
      "learning_rate": 2.0875420875420878e-07,
      "loss": 0.3333,
      "step": 310
    },
    {
      "epoch": 0.0031414141414141416,
      "grad_norm": 0.11705981940031052,
      "learning_rate": 2.0942760942760944e-07,
      "loss": 0.8148,
      "step": 311
    },
    {
      "epoch": 0.0031515151515151517,
      "grad_norm": 0.11102063953876495,
      "learning_rate": 2.1010101010101013e-07,
      "loss": 1.0548,
      "step": 312
    },
    {
      "epoch": 0.0031616161616161618,
      "grad_norm": 0.17739541828632355,
      "learning_rate": 2.107744107744108e-07,
      "loss": 0.4264,
      "step": 313
    },
    {
      "epoch": 0.003171717171717172,
      "grad_norm": 0.11822672933340073,
      "learning_rate": 2.1144781144781145e-07,
      "loss": 0.5407,
      "step": 314
    },
    {
      "epoch": 0.003181818181818182,
      "grad_norm": 0.18283721804618835,
      "learning_rate": 2.1212121212121216e-07,
      "loss": 0.4646,
      "step": 315
    },
    {
      "epoch": 0.003191919191919192,
      "grad_norm": 0.18974770605564117,
      "learning_rate": 2.1279461279461282e-07,
      "loss": 0.5781,
      "step": 316
    },
    {
      "epoch": 0.003202020202020202,
      "grad_norm": 0.15842516720294952,
      "learning_rate": 2.1346801346801348e-07,
      "loss": 0.6075,
      "step": 317
    },
    {
      "epoch": 0.003212121212121212,
      "grad_norm": 0.24217364192008972,
      "learning_rate": 2.1414141414141414e-07,
      "loss": 0.648,
      "step": 318
    },
    {
      "epoch": 0.0032222222222222222,
      "grad_norm": 0.1644904464483261,
      "learning_rate": 2.1481481481481483e-07,
      "loss": 0.6703,
      "step": 319
    },
    {
      "epoch": 0.0032323232323232323,
      "grad_norm": 0.10826272517442703,
      "learning_rate": 2.154882154882155e-07,
      "loss": 0.5629,
      "step": 320
    },
    {
      "epoch": 0.0032424242424242424,
      "grad_norm": 0.10423333942890167,
      "learning_rate": 2.1616161616161617e-07,
      "loss": 0.696,
      "step": 321
    },
    {
      "epoch": 0.0032525252525252525,
      "grad_norm": 0.20396895706653595,
      "learning_rate": 2.1683501683501686e-07,
      "loss": 0.8857,
      "step": 322
    },
    {
      "epoch": 0.0032626262626262626,
      "grad_norm": 0.1758105605840683,
      "learning_rate": 2.1750841750841752e-07,
      "loss": 1.014,
      "step": 323
    },
    {
      "epoch": 0.0032727272727272726,
      "grad_norm": 0.16695784032344818,
      "learning_rate": 2.181818181818182e-07,
      "loss": 0.7237,
      "step": 324
    },
    {
      "epoch": 0.0032828282828282827,
      "grad_norm": 0.15028288960456848,
      "learning_rate": 2.188552188552189e-07,
      "loss": 0.7686,
      "step": 325
    },
    {
      "epoch": 0.003292929292929293,
      "grad_norm": 0.1918318122625351,
      "learning_rate": 2.1952861952861955e-07,
      "loss": 0.6089,
      "step": 326
    },
    {
      "epoch": 0.003303030303030303,
      "grad_norm": 0.15289102494716644,
      "learning_rate": 2.202020202020202e-07,
      "loss": 0.6325,
      "step": 327
    },
    {
      "epoch": 0.003313131313131313,
      "grad_norm": 0.1404198855161667,
      "learning_rate": 2.2087542087542087e-07,
      "loss": 0.6938,
      "step": 328
    },
    {
      "epoch": 0.003323232323232323,
      "grad_norm": 0.11673679202795029,
      "learning_rate": 2.2154882154882158e-07,
      "loss": 0.501,
      "step": 329
    },
    {
      "epoch": 0.0033333333333333335,
      "grad_norm": 0.149989053606987,
      "learning_rate": 2.2222222222222224e-07,
      "loss": 0.4559,
      "step": 330
    },
    {
      "epoch": 0.0033434343434343436,
      "grad_norm": 0.12037955969572067,
      "learning_rate": 2.228956228956229e-07,
      "loss": 0.8045,
      "step": 331
    },
    {
      "epoch": 0.0033535353535353537,
      "grad_norm": 0.33125820755958557,
      "learning_rate": 2.235690235690236e-07,
      "loss": 0.7364,
      "step": 332
    },
    {
      "epoch": 0.003363636363636364,
      "grad_norm": 0.1593979299068451,
      "learning_rate": 2.2424242424242425e-07,
      "loss": 0.8669,
      "step": 333
    },
    {
      "epoch": 0.003373737373737374,
      "grad_norm": 0.07408119738101959,
      "learning_rate": 2.2491582491582494e-07,
      "loss": 0.2312,
      "step": 334
    },
    {
      "epoch": 0.003383838383838384,
      "grad_norm": 0.15409991145133972,
      "learning_rate": 2.2558922558922562e-07,
      "loss": 0.6775,
      "step": 335
    },
    {
      "epoch": 0.003393939393939394,
      "grad_norm": 0.1689748913049698,
      "learning_rate": 2.2626262626262628e-07,
      "loss": 0.664,
      "step": 336
    },
    {
      "epoch": 0.003404040404040404,
      "grad_norm": 0.23001471161842346,
      "learning_rate": 2.2693602693602694e-07,
      "loss": 0.8926,
      "step": 337
    },
    {
      "epoch": 0.003414141414141414,
      "grad_norm": 0.11179772764444351,
      "learning_rate": 2.276094276094276e-07,
      "loss": 0.559,
      "step": 338
    },
    {
      "epoch": 0.0034242424242424243,
      "grad_norm": 0.15261805057525635,
      "learning_rate": 2.2828282828282832e-07,
      "loss": 0.3799,
      "step": 339
    },
    {
      "epoch": 0.0034343434343434343,
      "grad_norm": 0.16223162412643433,
      "learning_rate": 2.2895622895622898e-07,
      "loss": 0.69,
      "step": 340
    },
    {
      "epoch": 0.0034444444444444444,
      "grad_norm": 0.11716701835393906,
      "learning_rate": 2.2962962962962964e-07,
      "loss": 1.021,
      "step": 341
    },
    {
      "epoch": 0.0034545454545454545,
      "grad_norm": 0.14976514875888824,
      "learning_rate": 2.3030303030303032e-07,
      "loss": 1.0283,
      "step": 342
    },
    {
      "epoch": 0.0034646464646464646,
      "grad_norm": 0.10318596661090851,
      "learning_rate": 2.3097643097643098e-07,
      "loss": 0.6062,
      "step": 343
    },
    {
      "epoch": 0.0034747474747474747,
      "grad_norm": 0.16923652589321136,
      "learning_rate": 2.3164983164983167e-07,
      "loss": 0.5851,
      "step": 344
    },
    {
      "epoch": 0.0034848484848484847,
      "grad_norm": 0.12339730560779572,
      "learning_rate": 2.3232323232323235e-07,
      "loss": 0.3884,
      "step": 345
    },
    {
      "epoch": 0.003494949494949495,
      "grad_norm": 0.12861239910125732,
      "learning_rate": 2.3299663299663301e-07,
      "loss": 0.6877,
      "step": 346
    },
    {
      "epoch": 0.003505050505050505,
      "grad_norm": 0.24540071189403534,
      "learning_rate": 2.3367003367003367e-07,
      "loss": 0.9392,
      "step": 347
    },
    {
      "epoch": 0.003515151515151515,
      "grad_norm": 0.17516367137432098,
      "learning_rate": 2.343434343434344e-07,
      "loss": 0.7421,
      "step": 348
    },
    {
      "epoch": 0.003525252525252525,
      "grad_norm": 0.2443842589855194,
      "learning_rate": 2.3501683501683505e-07,
      "loss": 0.7103,
      "step": 349
    },
    {
      "epoch": 0.0035353535353535356,
      "grad_norm": 0.22496937215328217,
      "learning_rate": 2.356902356902357e-07,
      "loss": 0.6498,
      "step": 350
    },
    {
      "epoch": 0.0035454545454545456,
      "grad_norm": 0.16104678809642792,
      "learning_rate": 2.3636363636363637e-07,
      "loss": 0.453,
      "step": 351
    },
    {
      "epoch": 0.0035555555555555557,
      "grad_norm": 0.16765326261520386,
      "learning_rate": 2.3703703703703705e-07,
      "loss": 0.8798,
      "step": 352
    },
    {
      "epoch": 0.003565656565656566,
      "grad_norm": 0.1571022868156433,
      "learning_rate": 2.3771043771043774e-07,
      "loss": 0.7801,
      "step": 353
    },
    {
      "epoch": 0.003575757575757576,
      "grad_norm": 0.12905815243721008,
      "learning_rate": 2.383838383838384e-07,
      "loss": 0.7407,
      "step": 354
    },
    {
      "epoch": 0.003585858585858586,
      "grad_norm": 0.11248265206813812,
      "learning_rate": 2.3905723905723906e-07,
      "loss": 0.4251,
      "step": 355
    },
    {
      "epoch": 0.003595959595959596,
      "grad_norm": 0.202085480093956,
      "learning_rate": 2.3973063973063975e-07,
      "loss": 0.5243,
      "step": 356
    },
    {
      "epoch": 0.003606060606060606,
      "grad_norm": 0.19674551486968994,
      "learning_rate": 2.4040404040404043e-07,
      "loss": 0.7791,
      "step": 357
    },
    {
      "epoch": 0.003616161616161616,
      "grad_norm": 0.1631637066602707,
      "learning_rate": 2.410774410774411e-07,
      "loss": 0.4792,
      "step": 358
    },
    {
      "epoch": 0.0036262626262626263,
      "grad_norm": 0.08968465030193329,
      "learning_rate": 2.4175084175084175e-07,
      "loss": 0.7222,
      "step": 359
    },
    {
      "epoch": 0.0036363636363636364,
      "grad_norm": 0.14514648914337158,
      "learning_rate": 2.4242424242424244e-07,
      "loss": 0.4012,
      "step": 360
    },
    {
      "epoch": 0.0036464646464646464,
      "grad_norm": 0.14130860567092896,
      "learning_rate": 2.430976430976431e-07,
      "loss": 0.4001,
      "step": 361
    },
    {
      "epoch": 0.0036565656565656565,
      "grad_norm": 0.1245151087641716,
      "learning_rate": 2.4377104377104376e-07,
      "loss": 0.6367,
      "step": 362
    },
    {
      "epoch": 0.0036666666666666666,
      "grad_norm": 0.1186862587928772,
      "learning_rate": 2.444444444444445e-07,
      "loss": 0.7244,
      "step": 363
    },
    {
      "epoch": 0.0036767676767676767,
      "grad_norm": 0.16872182488441467,
      "learning_rate": 2.4511784511784513e-07,
      "loss": 0.3971,
      "step": 364
    },
    {
      "epoch": 0.0036868686868686868,
      "grad_norm": 0.16498731076717377,
      "learning_rate": 2.457912457912458e-07,
      "loss": 0.6365,
      "step": 365
    },
    {
      "epoch": 0.003696969696969697,
      "grad_norm": 0.1786889135837555,
      "learning_rate": 2.4646464646464645e-07,
      "loss": 0.6433,
      "step": 366
    },
    {
      "epoch": 0.003707070707070707,
      "grad_norm": 0.13475115597248077,
      "learning_rate": 2.4713804713804714e-07,
      "loss": 0.705,
      "step": 367
    },
    {
      "epoch": 0.003717171717171717,
      "grad_norm": 0.3178465962409973,
      "learning_rate": 2.478114478114478e-07,
      "loss": 1.0825,
      "step": 368
    },
    {
      "epoch": 0.003727272727272727,
      "grad_norm": 0.11728927493095398,
      "learning_rate": 2.484848484848485e-07,
      "loss": 0.6546,
      "step": 369
    },
    {
      "epoch": 0.0037373737373737376,
      "grad_norm": 0.45335617661476135,
      "learning_rate": 2.491582491582492e-07,
      "loss": 1.0028,
      "step": 370
    },
    {
      "epoch": 0.0037474747474747477,
      "grad_norm": 0.14811988174915314,
      "learning_rate": 2.4983164983164983e-07,
      "loss": 0.6369,
      "step": 371
    },
    {
      "epoch": 0.0037575757575757577,
      "grad_norm": 0.10082532465457916,
      "learning_rate": 2.505050505050505e-07,
      "loss": 0.8341,
      "step": 372
    },
    {
      "epoch": 0.003767676767676768,
      "grad_norm": 0.13109536468982697,
      "learning_rate": 2.511784511784512e-07,
      "loss": 0.2824,
      "step": 373
    },
    {
      "epoch": 0.003777777777777778,
      "grad_norm": 0.13977183401584625,
      "learning_rate": 2.518518518518519e-07,
      "loss": 0.8384,
      "step": 374
    },
    {
      "epoch": 0.003787878787878788,
      "grad_norm": 0.17049835622310638,
      "learning_rate": 2.525252525252525e-07,
      "loss": 1.0075,
      "step": 375
    },
    {
      "epoch": 0.003797979797979798,
      "grad_norm": 0.23236578702926636,
      "learning_rate": 2.531986531986532e-07,
      "loss": 0.5532,
      "step": 376
    },
    {
      "epoch": 0.003808080808080808,
      "grad_norm": 0.12426383793354034,
      "learning_rate": 2.538720538720539e-07,
      "loss": 0.5415,
      "step": 377
    },
    {
      "epoch": 0.0038181818181818182,
      "grad_norm": 0.11553562432527542,
      "learning_rate": 2.545454545454546e-07,
      "loss": 0.7096,
      "step": 378
    },
    {
      "epoch": 0.0038282828282828283,
      "grad_norm": 0.15547001361846924,
      "learning_rate": 2.552188552188552e-07,
      "loss": 0.9212,
      "step": 379
    },
    {
      "epoch": 0.0038383838383838384,
      "grad_norm": 0.10848614573478699,
      "learning_rate": 2.558922558922559e-07,
      "loss": 0.5827,
      "step": 380
    },
    {
      "epoch": 0.0038484848484848485,
      "grad_norm": 0.1304389387369156,
      "learning_rate": 2.565656565656566e-07,
      "loss": 0.6421,
      "step": 381
    },
    {
      "epoch": 0.0038585858585858585,
      "grad_norm": 0.15866295993328094,
      "learning_rate": 2.572390572390573e-07,
      "loss": 0.5802,
      "step": 382
    },
    {
      "epoch": 0.0038686868686868686,
      "grad_norm": 0.12653757631778717,
      "learning_rate": 2.5791245791245796e-07,
      "loss": 0.5616,
      "step": 383
    },
    {
      "epoch": 0.0038787878787878787,
      "grad_norm": 0.18200041353702545,
      "learning_rate": 2.585858585858586e-07,
      "loss": 0.6614,
      "step": 384
    },
    {
      "epoch": 0.0038888888888888888,
      "grad_norm": 0.17755785584449768,
      "learning_rate": 2.592592592592593e-07,
      "loss": 0.636,
      "step": 385
    },
    {
      "epoch": 0.003898989898989899,
      "grad_norm": 0.19943676888942719,
      "learning_rate": 2.599326599326599e-07,
      "loss": 0.6306,
      "step": 386
    },
    {
      "epoch": 0.003909090909090909,
      "grad_norm": 0.28843754529953003,
      "learning_rate": 2.6060606060606065e-07,
      "loss": 0.9517,
      "step": 387
    },
    {
      "epoch": 0.0039191919191919194,
      "grad_norm": 0.08936092257499695,
      "learning_rate": 2.612794612794613e-07,
      "loss": 0.5863,
      "step": 388
    },
    {
      "epoch": 0.003929292929292929,
      "grad_norm": 0.1328953355550766,
      "learning_rate": 2.6195286195286197e-07,
      "loss": 0.8874,
      "step": 389
    },
    {
      "epoch": 0.00393939393939394,
      "grad_norm": 0.2393723577260971,
      "learning_rate": 2.6262626262626266e-07,
      "loss": 0.5041,
      "step": 390
    },
    {
      "epoch": 0.003949494949494949,
      "grad_norm": 0.101629339158535,
      "learning_rate": 2.632996632996633e-07,
      "loss": 0.6945,
      "step": 391
    },
    {
      "epoch": 0.00395959595959596,
      "grad_norm": 0.12947675585746765,
      "learning_rate": 2.63973063973064e-07,
      "loss": 0.4291,
      "step": 392
    },
    {
      "epoch": 0.003969696969696969,
      "grad_norm": 0.21417807042598724,
      "learning_rate": 2.6464646464646467e-07,
      "loss": 0.6674,
      "step": 393
    },
    {
      "epoch": 0.00397979797979798,
      "grad_norm": 0.24576729536056519,
      "learning_rate": 2.6531986531986535e-07,
      "loss": 0.9658,
      "step": 394
    },
    {
      "epoch": 0.00398989898989899,
      "grad_norm": 0.15503890812397003,
      "learning_rate": 2.65993265993266e-07,
      "loss": 0.6548,
      "step": 395
    },
    {
      "epoch": 0.004,
      "grad_norm": 0.09228813648223877,
      "learning_rate": 2.666666666666667e-07,
      "loss": 0.7041,
      "step": 396
    },
    {
      "epoch": 0.00401010101010101,
      "grad_norm": 0.2111242115497589,
      "learning_rate": 2.6734006734006736e-07,
      "loss": 0.5811,
      "step": 397
    },
    {
      "epoch": 0.00402020202020202,
      "grad_norm": 0.12827838957309723,
      "learning_rate": 2.6801346801346805e-07,
      "loss": 0.6933,
      "step": 398
    },
    {
      "epoch": 0.00403030303030303,
      "grad_norm": 0.13268403708934784,
      "learning_rate": 2.686868686868687e-07,
      "loss": 0.3565,
      "step": 399
    },
    {
      "epoch": 0.00404040404040404,
      "grad_norm": 0.1233651340007782,
      "learning_rate": 2.6936026936026936e-07,
      "loss": 0.8133,
      "step": 400
    },
    {
      "epoch": 0.004050505050505051,
      "grad_norm": 0.21648474037647247,
      "learning_rate": 2.7003367003367005e-07,
      "loss": 1.0913,
      "step": 401
    },
    {
      "epoch": 0.0040606060606060606,
      "grad_norm": 0.11321082711219788,
      "learning_rate": 2.7070707070707074e-07,
      "loss": 0.5761,
      "step": 402
    },
    {
      "epoch": 0.004070707070707071,
      "grad_norm": 0.1027834415435791,
      "learning_rate": 2.713804713804714e-07,
      "loss": 0.4312,
      "step": 403
    },
    {
      "epoch": 0.004080808080808081,
      "grad_norm": 0.13254645466804504,
      "learning_rate": 2.7205387205387206e-07,
      "loss": 0.7391,
      "step": 404
    },
    {
      "epoch": 0.004090909090909091,
      "grad_norm": 0.12685419619083405,
      "learning_rate": 2.7272727272727274e-07,
      "loss": 0.5199,
      "step": 405
    },
    {
      "epoch": 0.004101010101010101,
      "grad_norm": 0.19109877943992615,
      "learning_rate": 2.7340067340067343e-07,
      "loss": 0.5956,
      "step": 406
    },
    {
      "epoch": 0.004111111111111111,
      "grad_norm": 0.14508600533008575,
      "learning_rate": 2.740740740740741e-07,
      "loss": 0.7445,
      "step": 407
    },
    {
      "epoch": 0.004121212121212121,
      "grad_norm": 0.30057117342948914,
      "learning_rate": 2.7474747474747475e-07,
      "loss": 0.7073,
      "step": 408
    },
    {
      "epoch": 0.0041313131313131315,
      "grad_norm": 0.13710542023181915,
      "learning_rate": 2.7542087542087544e-07,
      "loss": 0.6791,
      "step": 409
    },
    {
      "epoch": 0.004141414141414141,
      "grad_norm": 0.12441407889127731,
      "learning_rate": 2.760942760942761e-07,
      "loss": 0.3976,
      "step": 410
    },
    {
      "epoch": 0.004151515151515152,
      "grad_norm": 0.2666926383972168,
      "learning_rate": 2.767676767676768e-07,
      "loss": 0.8594,
      "step": 411
    },
    {
      "epoch": 0.004161616161616161,
      "grad_norm": 0.26664379239082336,
      "learning_rate": 2.774410774410775e-07,
      "loss": 0.9293,
      "step": 412
    },
    {
      "epoch": 0.004171717171717172,
      "grad_norm": 0.17389802634716034,
      "learning_rate": 2.7811447811447813e-07,
      "loss": 0.7557,
      "step": 413
    },
    {
      "epoch": 0.0041818181818181815,
      "grad_norm": 0.18922531604766846,
      "learning_rate": 2.787878787878788e-07,
      "loss": 0.459,
      "step": 414
    },
    {
      "epoch": 0.004191919191919192,
      "grad_norm": 0.17883789539337158,
      "learning_rate": 2.7946127946127945e-07,
      "loss": 0.6284,
      "step": 415
    },
    {
      "epoch": 0.004202020202020202,
      "grad_norm": 0.3215314447879791,
      "learning_rate": 2.801346801346802e-07,
      "loss": 0.7342,
      "step": 416
    },
    {
      "epoch": 0.004212121212121212,
      "grad_norm": 0.11818966269493103,
      "learning_rate": 2.808080808080808e-07,
      "loss": 0.4478,
      "step": 417
    },
    {
      "epoch": 0.004222222222222222,
      "grad_norm": 0.11943060159683228,
      "learning_rate": 2.814814814814815e-07,
      "loss": 0.4905,
      "step": 418
    },
    {
      "epoch": 0.004232323232323232,
      "grad_norm": 0.12265429645776749,
      "learning_rate": 2.8215488215488214e-07,
      "loss": 0.7928,
      "step": 419
    },
    {
      "epoch": 0.004242424242424243,
      "grad_norm": 0.09403622150421143,
      "learning_rate": 2.828282828282829e-07,
      "loss": 0.8067,
      "step": 420
    },
    {
      "epoch": 0.0042525252525252525,
      "grad_norm": 0.13576817512512207,
      "learning_rate": 2.835016835016835e-07,
      "loss": 0.5169,
      "step": 421
    },
    {
      "epoch": 0.004262626262626263,
      "grad_norm": 0.15767188370227814,
      "learning_rate": 2.841750841750842e-07,
      "loss": 0.5635,
      "step": 422
    },
    {
      "epoch": 0.004272727272727273,
      "grad_norm": 0.25756120681762695,
      "learning_rate": 2.848484848484849e-07,
      "loss": 0.5218,
      "step": 423
    },
    {
      "epoch": 0.004282828282828283,
      "grad_norm": 0.2973906397819519,
      "learning_rate": 2.855218855218855e-07,
      "loss": 0.8717,
      "step": 424
    },
    {
      "epoch": 0.004292929292929293,
      "grad_norm": 0.13511012494564056,
      "learning_rate": 2.8619528619528626e-07,
      "loss": 0.3593,
      "step": 425
    },
    {
      "epoch": 0.004303030303030303,
      "grad_norm": 0.16861054301261902,
      "learning_rate": 2.868686868686869e-07,
      "loss": 0.4605,
      "step": 426
    },
    {
      "epoch": 0.004313131313131313,
      "grad_norm": 0.1393815577030182,
      "learning_rate": 2.875420875420876e-07,
      "loss": 0.789,
      "step": 427
    },
    {
      "epoch": 0.0043232323232323235,
      "grad_norm": 0.2181265652179718,
      "learning_rate": 2.882154882154882e-07,
      "loss": 1.1944,
      "step": 428
    },
    {
      "epoch": 0.004333333333333333,
      "grad_norm": 0.2944682836532593,
      "learning_rate": 2.888888888888889e-07,
      "loss": 0.7032,
      "step": 429
    },
    {
      "epoch": 0.004343434343434344,
      "grad_norm": 0.22473780810832977,
      "learning_rate": 2.895622895622896e-07,
      "loss": 1.0635,
      "step": 430
    },
    {
      "epoch": 0.004353535353535353,
      "grad_norm": 0.19472767412662506,
      "learning_rate": 2.9023569023569027e-07,
      "loss": 0.8632,
      "step": 431
    },
    {
      "epoch": 0.004363636363636364,
      "grad_norm": 0.1292606145143509,
      "learning_rate": 2.9090909090909096e-07,
      "loss": 0.6408,
      "step": 432
    },
    {
      "epoch": 0.0043737373737373735,
      "grad_norm": 0.11461132019758224,
      "learning_rate": 2.915824915824916e-07,
      "loss": 0.3834,
      "step": 433
    },
    {
      "epoch": 0.004383838383838384,
      "grad_norm": 0.27084288001060486,
      "learning_rate": 2.922558922558923e-07,
      "loss": 1.1768,
      "step": 434
    },
    {
      "epoch": 0.004393939393939394,
      "grad_norm": 0.14314484596252441,
      "learning_rate": 2.9292929292929296e-07,
      "loss": 0.7021,
      "step": 435
    },
    {
      "epoch": 0.004404040404040404,
      "grad_norm": 0.19691279530525208,
      "learning_rate": 2.9360269360269365e-07,
      "loss": 0.5712,
      "step": 436
    },
    {
      "epoch": 0.004414141414141414,
      "grad_norm": 0.2632223963737488,
      "learning_rate": 2.942760942760943e-07,
      "loss": 0.9068,
      "step": 437
    },
    {
      "epoch": 0.004424242424242424,
      "grad_norm": 0.13978642225265503,
      "learning_rate": 2.9494949494949497e-07,
      "loss": 0.3788,
      "step": 438
    },
    {
      "epoch": 0.004434343434343434,
      "grad_norm": 0.13768866658210754,
      "learning_rate": 2.9562289562289566e-07,
      "loss": 0.4313,
      "step": 439
    },
    {
      "epoch": 0.0044444444444444444,
      "grad_norm": 0.2768518924713135,
      "learning_rate": 2.9629629629629634e-07,
      "loss": 0.7937,
      "step": 440
    },
    {
      "epoch": 0.004454545454545455,
      "grad_norm": 0.10735063999891281,
      "learning_rate": 2.96969696969697e-07,
      "loss": 0.8063,
      "step": 441
    },
    {
      "epoch": 0.004464646464646465,
      "grad_norm": 0.13772639632225037,
      "learning_rate": 2.9764309764309766e-07,
      "loss": 0.5558,
      "step": 442
    },
    {
      "epoch": 0.004474747474747475,
      "grad_norm": 0.1646767109632492,
      "learning_rate": 2.9831649831649835e-07,
      "loss": 1.0104,
      "step": 443
    },
    {
      "epoch": 0.004484848484848485,
      "grad_norm": 0.10981932282447815,
      "learning_rate": 2.9898989898989904e-07,
      "loss": 0.7643,
      "step": 444
    },
    {
      "epoch": 0.004494949494949495,
      "grad_norm": 0.1357487440109253,
      "learning_rate": 2.996632996632997e-07,
      "loss": 0.652,
      "step": 445
    },
    {
      "epoch": 0.004505050505050505,
      "grad_norm": 0.100472092628479,
      "learning_rate": 3.0033670033670036e-07,
      "loss": 0.5784,
      "step": 446
    },
    {
      "epoch": 0.004515151515151515,
      "grad_norm": 0.21157322824001312,
      "learning_rate": 3.0101010101010104e-07,
      "loss": 0.7997,
      "step": 447
    },
    {
      "epoch": 0.004525252525252525,
      "grad_norm": 0.2441110759973526,
      "learning_rate": 3.016835016835017e-07,
      "loss": 1.0178,
      "step": 448
    },
    {
      "epoch": 0.004535353535353536,
      "grad_norm": 0.463589072227478,
      "learning_rate": 3.023569023569024e-07,
      "loss": 0.8414,
      "step": 449
    },
    {
      "epoch": 0.004545454545454545,
      "grad_norm": 0.26312845945358276,
      "learning_rate": 3.0303030303030305e-07,
      "loss": 0.5057,
      "step": 450
    },
    {
      "epoch": 0.004555555555555556,
      "grad_norm": 0.1342857927083969,
      "learning_rate": 3.0370370370370374e-07,
      "loss": 1.0007,
      "step": 451
    },
    {
      "epoch": 0.004565656565656565,
      "grad_norm": 0.14444246888160706,
      "learning_rate": 3.043771043771044e-07,
      "loss": 0.4005,
      "step": 452
    },
    {
      "epoch": 0.004575757575757576,
      "grad_norm": 0.14244404435157776,
      "learning_rate": 3.0505050505050505e-07,
      "loss": 0.5378,
      "step": 453
    },
    {
      "epoch": 0.0045858585858585856,
      "grad_norm": 0.13148295879364014,
      "learning_rate": 3.0572390572390574e-07,
      "loss": 0.7026,
      "step": 454
    },
    {
      "epoch": 0.004595959595959596,
      "grad_norm": 0.19108499586582184,
      "learning_rate": 3.0639730639730643e-07,
      "loss": 0.4546,
      "step": 455
    },
    {
      "epoch": 0.004606060606060606,
      "grad_norm": 0.12497186660766602,
      "learning_rate": 3.070707070707071e-07,
      "loss": 0.5483,
      "step": 456
    },
    {
      "epoch": 0.004616161616161616,
      "grad_norm": 0.13883309066295624,
      "learning_rate": 3.0774410774410775e-07,
      "loss": 0.622,
      "step": 457
    },
    {
      "epoch": 0.004626262626262626,
      "grad_norm": 0.15720850229263306,
      "learning_rate": 3.0841750841750843e-07,
      "loss": 0.9386,
      "step": 458
    },
    {
      "epoch": 0.004636363636363636,
      "grad_norm": 0.2355542629957199,
      "learning_rate": 3.090909090909091e-07,
      "loss": 1.3094,
      "step": 459
    },
    {
      "epoch": 0.004646464646464647,
      "grad_norm": 0.1822739839553833,
      "learning_rate": 3.097643097643098e-07,
      "loss": 0.4689,
      "step": 460
    },
    {
      "epoch": 0.0046565656565656565,
      "grad_norm": 0.11716671288013458,
      "learning_rate": 3.1043771043771044e-07,
      "loss": 0.4056,
      "step": 461
    },
    {
      "epoch": 0.004666666666666667,
      "grad_norm": 0.12269165366888046,
      "learning_rate": 3.111111111111111e-07,
      "loss": 0.3723,
      "step": 462
    },
    {
      "epoch": 0.004676767676767677,
      "grad_norm": 0.10287479311227798,
      "learning_rate": 3.117845117845118e-07,
      "loss": 0.6808,
      "step": 463
    },
    {
      "epoch": 0.004686868686868687,
      "grad_norm": 0.2517798840999603,
      "learning_rate": 3.124579124579125e-07,
      "loss": 0.9468,
      "step": 464
    },
    {
      "epoch": 0.004696969696969697,
      "grad_norm": 0.14934490621089935,
      "learning_rate": 3.131313131313132e-07,
      "loss": 0.5955,
      "step": 465
    },
    {
      "epoch": 0.004707070707070707,
      "grad_norm": 0.14701074361801147,
      "learning_rate": 3.1380471380471387e-07,
      "loss": 0.909,
      "step": 466
    },
    {
      "epoch": 0.004717171717171717,
      "grad_norm": 0.13604044914245605,
      "learning_rate": 3.144781144781145e-07,
      "loss": 0.3754,
      "step": 467
    },
    {
      "epoch": 0.0047272727272727275,
      "grad_norm": 0.11543837189674377,
      "learning_rate": 3.151515151515152e-07,
      "loss": 0.5406,
      "step": 468
    },
    {
      "epoch": 0.004737373737373737,
      "grad_norm": 0.11114924401044846,
      "learning_rate": 3.158249158249158e-07,
      "loss": 0.7296,
      "step": 469
    },
    {
      "epoch": 0.004747474747474748,
      "grad_norm": 0.14702211320400238,
      "learning_rate": 3.164983164983165e-07,
      "loss": 0.7442,
      "step": 470
    },
    {
      "epoch": 0.004757575757575757,
      "grad_norm": 0.13390879333019257,
      "learning_rate": 3.1717171717171725e-07,
      "loss": 0.9012,
      "step": 471
    },
    {
      "epoch": 0.004767676767676768,
      "grad_norm": 0.11672838777303696,
      "learning_rate": 3.178451178451179e-07,
      "loss": 0.6334,
      "step": 472
    },
    {
      "epoch": 0.0047777777777777775,
      "grad_norm": 0.12395945191383362,
      "learning_rate": 3.1851851851851857e-07,
      "loss": 0.6767,
      "step": 473
    },
    {
      "epoch": 0.004787878787878788,
      "grad_norm": 0.10083991289138794,
      "learning_rate": 3.191919191919192e-07,
      "loss": 0.7406,
      "step": 474
    },
    {
      "epoch": 0.004797979797979798,
      "grad_norm": 0.19114965200424194,
      "learning_rate": 3.198653198653199e-07,
      "loss": 0.5715,
      "step": 475
    },
    {
      "epoch": 0.004808080808080808,
      "grad_norm": 0.13508890569210052,
      "learning_rate": 3.205387205387206e-07,
      "loss": 0.848,
      "step": 476
    },
    {
      "epoch": 0.004818181818181818,
      "grad_norm": 0.08792290836572647,
      "learning_rate": 3.212121212121212e-07,
      "loss": 0.3268,
      "step": 477
    },
    {
      "epoch": 0.004828282828282828,
      "grad_norm": 0.09074530005455017,
      "learning_rate": 3.2188552188552195e-07,
      "loss": 0.607,
      "step": 478
    },
    {
      "epoch": 0.004838383838383838,
      "grad_norm": 0.16792424023151398,
      "learning_rate": 3.225589225589226e-07,
      "loss": 0.646,
      "step": 479
    },
    {
      "epoch": 0.0048484848484848485,
      "grad_norm": 0.11226139962673187,
      "learning_rate": 3.2323232323232327e-07,
      "loss": 0.597,
      "step": 480
    },
    {
      "epoch": 0.004858585858585859,
      "grad_norm": 0.13190165162086487,
      "learning_rate": 3.2390572390572396e-07,
      "loss": 0.6114,
      "step": 481
    },
    {
      "epoch": 0.004868686868686869,
      "grad_norm": 0.40290936827659607,
      "learning_rate": 3.245791245791246e-07,
      "loss": 0.9741,
      "step": 482
    },
    {
      "epoch": 0.004878787878787879,
      "grad_norm": 0.14044441282749176,
      "learning_rate": 3.252525252525253e-07,
      "loss": 0.4409,
      "step": 483
    },
    {
      "epoch": 0.004888888888888889,
      "grad_norm": 0.18750976026058197,
      "learning_rate": 3.259259259259259e-07,
      "loss": 0.6543,
      "step": 484
    },
    {
      "epoch": 0.004898989898989899,
      "grad_norm": 0.13956846296787262,
      "learning_rate": 3.2659932659932665e-07,
      "loss": 0.4569,
      "step": 485
    },
    {
      "epoch": 0.004909090909090909,
      "grad_norm": 0.20987625420093536,
      "learning_rate": 3.2727272727272733e-07,
      "loss": 0.6097,
      "step": 486
    },
    {
      "epoch": 0.0049191919191919195,
      "grad_norm": 0.1602172553539276,
      "learning_rate": 3.2794612794612797e-07,
      "loss": 0.6726,
      "step": 487
    },
    {
      "epoch": 0.004929292929292929,
      "grad_norm": 0.12408782541751862,
      "learning_rate": 3.2861952861952865e-07,
      "loss": 0.4514,
      "step": 488
    },
    {
      "epoch": 0.00493939393939394,
      "grad_norm": 0.2847571074962616,
      "learning_rate": 3.292929292929293e-07,
      "loss": 0.5009,
      "step": 489
    },
    {
      "epoch": 0.004949494949494949,
      "grad_norm": 0.2595817744731903,
      "learning_rate": 3.2996632996633e-07,
      "loss": 1.0679,
      "step": 490
    },
    {
      "epoch": 0.00495959595959596,
      "grad_norm": 0.15185126662254333,
      "learning_rate": 3.306397306397307e-07,
      "loss": 0.9746,
      "step": 491
    },
    {
      "epoch": 0.004969696969696969,
      "grad_norm": 0.14324431121349335,
      "learning_rate": 3.3131313131313135e-07,
      "loss": 0.6679,
      "step": 492
    },
    {
      "epoch": 0.00497979797979798,
      "grad_norm": 0.18256419897079468,
      "learning_rate": 3.3198653198653203e-07,
      "loss": 0.8658,
      "step": 493
    },
    {
      "epoch": 0.00498989898989899,
      "grad_norm": 0.2690894603729248,
      "learning_rate": 3.3265993265993267e-07,
      "loss": 0.7004,
      "step": 494
    },
    {
      "epoch": 0.005,
      "grad_norm": 0.1080450713634491,
      "learning_rate": 3.3333333333333335e-07,
      "loss": 0.5663,
      "step": 495
    },
    {
      "epoch": 0.00501010101010101,
      "grad_norm": 0.27027663588523865,
      "learning_rate": 3.3400673400673404e-07,
      "loss": 0.8782,
      "step": 496
    },
    {
      "epoch": 0.00502020202020202,
      "grad_norm": 0.36438247561454773,
      "learning_rate": 3.3468013468013467e-07,
      "loss": 0.8013,
      "step": 497
    },
    {
      "epoch": 0.00503030303030303,
      "grad_norm": 0.17969074845314026,
      "learning_rate": 3.353535353535354e-07,
      "loss": 1.173,
      "step": 498
    },
    {
      "epoch": 0.00504040404040404,
      "grad_norm": 0.11846502125263214,
      "learning_rate": 3.3602693602693605e-07,
      "loss": 0.5791,
      "step": 499
    },
    {
      "epoch": 0.005050505050505051,
      "grad_norm": 0.3353824019432068,
      "learning_rate": 3.3670033670033673e-07,
      "loss": 1.0782,
      "step": 500
    },
    {
      "epoch": 0.005060606060606061,
      "grad_norm": 0.13801541924476624,
      "learning_rate": 3.373737373737374e-07,
      "loss": 0.5604,
      "step": 501
    },
    {
      "epoch": 0.005070707070707071,
      "grad_norm": 0.2031181901693344,
      "learning_rate": 3.3804713804713805e-07,
      "loss": 0.9803,
      "step": 502
    },
    {
      "epoch": 0.005080808080808081,
      "grad_norm": 0.19063180685043335,
      "learning_rate": 3.3872053872053874e-07,
      "loss": 0.6888,
      "step": 503
    },
    {
      "epoch": 0.005090909090909091,
      "grad_norm": 0.16252820193767548,
      "learning_rate": 3.393939393939395e-07,
      "loss": 0.4796,
      "step": 504
    },
    {
      "epoch": 0.005101010101010101,
      "grad_norm": 0.12750287353992462,
      "learning_rate": 3.400673400673401e-07,
      "loss": 0.6719,
      "step": 505
    },
    {
      "epoch": 0.005111111111111111,
      "grad_norm": 0.11809466779232025,
      "learning_rate": 3.407407407407408e-07,
      "loss": 0.3996,
      "step": 506
    },
    {
      "epoch": 0.005121212121212121,
      "grad_norm": 0.18168337643146515,
      "learning_rate": 3.4141414141414143e-07,
      "loss": 0.5074,
      "step": 507
    },
    {
      "epoch": 0.005131313131313132,
      "grad_norm": 0.10864654183387756,
      "learning_rate": 3.420875420875421e-07,
      "loss": 0.5981,
      "step": 508
    },
    {
      "epoch": 0.005141414141414141,
      "grad_norm": 0.1380634903907776,
      "learning_rate": 3.427609427609428e-07,
      "loss": 0.891,
      "step": 509
    },
    {
      "epoch": 0.005151515151515152,
      "grad_norm": 0.12889902293682098,
      "learning_rate": 3.4343434343434344e-07,
      "loss": 0.7962,
      "step": 510
    },
    {
      "epoch": 0.005161616161616161,
      "grad_norm": 0.17377690970897675,
      "learning_rate": 3.441077441077442e-07,
      "loss": 0.4443,
      "step": 511
    },
    {
      "epoch": 0.005171717171717172,
      "grad_norm": 0.2456597089767456,
      "learning_rate": 3.447811447811448e-07,
      "loss": 0.6709,
      "step": 512
    },
    {
      "epoch": 0.0051818181818181815,
      "grad_norm": 0.1382887214422226,
      "learning_rate": 3.454545454545455e-07,
      "loss": 0.7737,
      "step": 513
    },
    {
      "epoch": 0.005191919191919192,
      "grad_norm": 0.1043071374297142,
      "learning_rate": 3.461279461279462e-07,
      "loss": 0.5503,
      "step": 514
    },
    {
      "epoch": 0.005202020202020202,
      "grad_norm": 0.1059623435139656,
      "learning_rate": 3.468013468013468e-07,
      "loss": 0.6345,
      "step": 515
    },
    {
      "epoch": 0.005212121212121212,
      "grad_norm": 0.10492607206106186,
      "learning_rate": 3.474747474747475e-07,
      "loss": 0.4871,
      "step": 516
    },
    {
      "epoch": 0.005222222222222222,
      "grad_norm": 0.24357391893863678,
      "learning_rate": 3.4814814814814814e-07,
      "loss": 0.5073,
      "step": 517
    },
    {
      "epoch": 0.005232323232323232,
      "grad_norm": 0.25998541712760925,
      "learning_rate": 3.488215488215489e-07,
      "loss": 1.4607,
      "step": 518
    },
    {
      "epoch": 0.005242424242424242,
      "grad_norm": 0.1442447155714035,
      "learning_rate": 3.4949494949494956e-07,
      "loss": 0.4254,
      "step": 519
    },
    {
      "epoch": 0.0052525252525252525,
      "grad_norm": 0.1472843587398529,
      "learning_rate": 3.501683501683502e-07,
      "loss": 0.6698,
      "step": 520
    },
    {
      "epoch": 0.005262626262626263,
      "grad_norm": 0.2176157534122467,
      "learning_rate": 3.508417508417509e-07,
      "loss": 0.5118,
      "step": 521
    },
    {
      "epoch": 0.005272727272727273,
      "grad_norm": 0.13187263906002045,
      "learning_rate": 3.515151515151515e-07,
      "loss": 0.7707,
      "step": 522
    },
    {
      "epoch": 0.005282828282828283,
      "grad_norm": 0.29814934730529785,
      "learning_rate": 3.521885521885522e-07,
      "loss": 0.8548,
      "step": 523
    },
    {
      "epoch": 0.005292929292929293,
      "grad_norm": 0.229204460978508,
      "learning_rate": 3.5286195286195294e-07,
      "loss": 1.0178,
      "step": 524
    },
    {
      "epoch": 0.005303030303030303,
      "grad_norm": 0.08567636460065842,
      "learning_rate": 3.535353535353536e-07,
      "loss": 0.7831,
      "step": 525
    },
    {
      "epoch": 0.005313131313131313,
      "grad_norm": 0.14868682622909546,
      "learning_rate": 3.5420875420875426e-07,
      "loss": 0.4026,
      "step": 526
    },
    {
      "epoch": 0.0053232323232323235,
      "grad_norm": 0.1362670212984085,
      "learning_rate": 3.548821548821549e-07,
      "loss": 0.5062,
      "step": 527
    },
    {
      "epoch": 0.005333333333333333,
      "grad_norm": 0.15848736464977264,
      "learning_rate": 3.555555555555556e-07,
      "loss": 0.7258,
      "step": 528
    },
    {
      "epoch": 0.005343434343434344,
      "grad_norm": 0.12444336712360382,
      "learning_rate": 3.5622895622895627e-07,
      "loss": 0.5499,
      "step": 529
    },
    {
      "epoch": 0.005353535353535353,
      "grad_norm": 0.12155825644731522,
      "learning_rate": 3.569023569023569e-07,
      "loss": 0.6756,
      "step": 530
    },
    {
      "epoch": 0.005363636363636364,
      "grad_norm": 0.15691277384757996,
      "learning_rate": 3.5757575757575764e-07,
      "loss": 0.5729,
      "step": 531
    },
    {
      "epoch": 0.0053737373737373735,
      "grad_norm": 0.0911225974559784,
      "learning_rate": 3.5824915824915827e-07,
      "loss": 0.5965,
      "step": 532
    },
    {
      "epoch": 0.005383838383838384,
      "grad_norm": 0.11368229985237122,
      "learning_rate": 3.5892255892255896e-07,
      "loss": 0.7767,
      "step": 533
    },
    {
      "epoch": 0.005393939393939394,
      "grad_norm": 0.12754246592521667,
      "learning_rate": 3.5959595959595965e-07,
      "loss": 0.5828,
      "step": 534
    },
    {
      "epoch": 0.005404040404040404,
      "grad_norm": 0.19864638149738312,
      "learning_rate": 3.602693602693603e-07,
      "loss": 0.7768,
      "step": 535
    },
    {
      "epoch": 0.005414141414141414,
      "grad_norm": 0.20521464943885803,
      "learning_rate": 3.6094276094276097e-07,
      "loss": 0.6792,
      "step": 536
    },
    {
      "epoch": 0.005424242424242424,
      "grad_norm": 0.17024964094161987,
      "learning_rate": 3.616161616161616e-07,
      "loss": 0.3853,
      "step": 537
    },
    {
      "epoch": 0.005434343434343434,
      "grad_norm": 0.16103899478912354,
      "learning_rate": 3.6228956228956234e-07,
      "loss": 0.4288,
      "step": 538
    },
    {
      "epoch": 0.0054444444444444445,
      "grad_norm": 0.2732447385787964,
      "learning_rate": 3.62962962962963e-07,
      "loss": 0.7294,
      "step": 539
    },
    {
      "epoch": 0.005454545454545455,
      "grad_norm": 0.1340901404619217,
      "learning_rate": 3.6363636363636366e-07,
      "loss": 0.2975,
      "step": 540
    },
    {
      "epoch": 0.005464646464646465,
      "grad_norm": 0.1907421350479126,
      "learning_rate": 3.6430976430976434e-07,
      "loss": 0.58,
      "step": 541
    },
    {
      "epoch": 0.005474747474747475,
      "grad_norm": 0.18364664912223816,
      "learning_rate": 3.64983164983165e-07,
      "loss": 0.467,
      "step": 542
    },
    {
      "epoch": 0.005484848484848485,
      "grad_norm": 0.16375203430652618,
      "learning_rate": 3.6565656565656566e-07,
      "loss": 0.4123,
      "step": 543
    },
    {
      "epoch": 0.005494949494949495,
      "grad_norm": 0.19028645753860474,
      "learning_rate": 3.663299663299664e-07,
      "loss": 0.6833,
      "step": 544
    },
    {
      "epoch": 0.005505050505050505,
      "grad_norm": 0.10536082088947296,
      "learning_rate": 3.6700336700336704e-07,
      "loss": 0.5519,
      "step": 545
    },
    {
      "epoch": 0.0055151515151515155,
      "grad_norm": 0.0946885421872139,
      "learning_rate": 3.676767676767677e-07,
      "loss": 0.7666,
      "step": 546
    },
    {
      "epoch": 0.005525252525252525,
      "grad_norm": 0.2419680655002594,
      "learning_rate": 3.6835016835016836e-07,
      "loss": 1.0039,
      "step": 547
    },
    {
      "epoch": 0.005535353535353536,
      "grad_norm": 0.2224803864955902,
      "learning_rate": 3.6902356902356904e-07,
      "loss": 0.62,
      "step": 548
    },
    {
      "epoch": 0.005545454545454545,
      "grad_norm": 0.15897619724273682,
      "learning_rate": 3.6969696969696973e-07,
      "loss": 0.423,
      "step": 549
    },
    {
      "epoch": 0.005555555555555556,
      "grad_norm": 0.1658053994178772,
      "learning_rate": 3.7037037037037036e-07,
      "loss": 0.3555,
      "step": 550
    },
    {
      "epoch": 0.005565656565656565,
      "grad_norm": 0.14714322984218597,
      "learning_rate": 3.710437710437711e-07,
      "loss": 0.3739,
      "step": 551
    },
    {
      "epoch": 0.005575757575757576,
      "grad_norm": 0.12005952000617981,
      "learning_rate": 3.717171717171718e-07,
      "loss": 0.6594,
      "step": 552
    },
    {
      "epoch": 0.005585858585858586,
      "grad_norm": 0.22553883492946625,
      "learning_rate": 3.723905723905724e-07,
      "loss": 0.5944,
      "step": 553
    },
    {
      "epoch": 0.005595959595959596,
      "grad_norm": 0.12936411798000336,
      "learning_rate": 3.730639730639731e-07,
      "loss": 0.5914,
      "step": 554
    },
    {
      "epoch": 0.005606060606060606,
      "grad_norm": 0.1598319113254547,
      "learning_rate": 3.7373737373737374e-07,
      "loss": 0.4379,
      "step": 555
    },
    {
      "epoch": 0.005616161616161616,
      "grad_norm": 0.14060816168785095,
      "learning_rate": 3.7441077441077443e-07,
      "loss": 0.3991,
      "step": 556
    },
    {
      "epoch": 0.005626262626262626,
      "grad_norm": 0.15312884747982025,
      "learning_rate": 3.7508417508417517e-07,
      "loss": 0.6569,
      "step": 557
    },
    {
      "epoch": 0.005636363636363636,
      "grad_norm": 0.13435818254947662,
      "learning_rate": 3.757575757575758e-07,
      "loss": 0.4526,
      "step": 558
    },
    {
      "epoch": 0.005646464646464646,
      "grad_norm": 0.12099702656269073,
      "learning_rate": 3.764309764309765e-07,
      "loss": 0.8087,
      "step": 559
    },
    {
      "epoch": 0.0056565656565656566,
      "grad_norm": 0.09497757256031036,
      "learning_rate": 3.771043771043771e-07,
      "loss": 0.7893,
      "step": 560
    },
    {
      "epoch": 0.005666666666666667,
      "grad_norm": 0.08845391124486923,
      "learning_rate": 3.777777777777778e-07,
      "loss": 0.4924,
      "step": 561
    },
    {
      "epoch": 0.005676767676767677,
      "grad_norm": 0.12997320294380188,
      "learning_rate": 3.784511784511785e-07,
      "loss": 0.5882,
      "step": 562
    },
    {
      "epoch": 0.005686868686868687,
      "grad_norm": 0.23584328591823578,
      "learning_rate": 3.7912457912457913e-07,
      "loss": 1.1815,
      "step": 563
    },
    {
      "epoch": 0.005696969696969697,
      "grad_norm": 0.11255262792110443,
      "learning_rate": 3.7979797979797987e-07,
      "loss": 0.5724,
      "step": 564
    },
    {
      "epoch": 0.005707070707070707,
      "grad_norm": 0.23739242553710938,
      "learning_rate": 3.804713804713805e-07,
      "loss": 1.0851,
      "step": 565
    },
    {
      "epoch": 0.005717171717171717,
      "grad_norm": 0.2143443077802658,
      "learning_rate": 3.811447811447812e-07,
      "loss": 1.004,
      "step": 566
    },
    {
      "epoch": 0.0057272727272727275,
      "grad_norm": 0.477901816368103,
      "learning_rate": 3.8181818181818187e-07,
      "loss": 1.0776,
      "step": 567
    },
    {
      "epoch": 0.005737373737373737,
      "grad_norm": 0.09820901602506638,
      "learning_rate": 3.824915824915825e-07,
      "loss": 0.5219,
      "step": 568
    },
    {
      "epoch": 0.005747474747474748,
      "grad_norm": 0.22752246260643005,
      "learning_rate": 3.831649831649832e-07,
      "loss": 0.8707,
      "step": 569
    },
    {
      "epoch": 0.005757575757575757,
      "grad_norm": 0.10951894521713257,
      "learning_rate": 3.838383838383838e-07,
      "loss": 0.9763,
      "step": 570
    },
    {
      "epoch": 0.005767676767676768,
      "grad_norm": 0.149433434009552,
      "learning_rate": 3.8451178451178457e-07,
      "loss": 0.61,
      "step": 571
    },
    {
      "epoch": 0.0057777777777777775,
      "grad_norm": 0.13694825768470764,
      "learning_rate": 3.8518518518518525e-07,
      "loss": 0.7508,
      "step": 572
    },
    {
      "epoch": 0.005787878787878788,
      "grad_norm": 0.11607912927865982,
      "learning_rate": 3.858585858585859e-07,
      "loss": 0.669,
      "step": 573
    },
    {
      "epoch": 0.005797979797979798,
      "grad_norm": 0.13178181648254395,
      "learning_rate": 3.8653198653198657e-07,
      "loss": 0.4063,
      "step": 574
    },
    {
      "epoch": 0.005808080808080808,
      "grad_norm": 0.2064627707004547,
      "learning_rate": 3.872053872053872e-07,
      "loss": 0.9798,
      "step": 575
    },
    {
      "epoch": 0.005818181818181818,
      "grad_norm": 0.17417451739311218,
      "learning_rate": 3.878787878787879e-07,
      "loss": 0.7093,
      "step": 576
    },
    {
      "epoch": 0.005828282828282828,
      "grad_norm": 0.178269624710083,
      "learning_rate": 3.8855218855218863e-07,
      "loss": 1.0143,
      "step": 577
    },
    {
      "epoch": 0.005838383838383838,
      "grad_norm": 0.22765055298805237,
      "learning_rate": 3.8922558922558926e-07,
      "loss": 0.3401,
      "step": 578
    },
    {
      "epoch": 0.0058484848484848485,
      "grad_norm": 0.216088205575943,
      "learning_rate": 3.8989898989898995e-07,
      "loss": 1.0217,
      "step": 579
    },
    {
      "epoch": 0.005858585858585859,
      "grad_norm": 0.11747288703918457,
      "learning_rate": 3.905723905723906e-07,
      "loss": 0.3593,
      "step": 580
    },
    {
      "epoch": 0.005868686868686869,
      "grad_norm": 0.15610386431217194,
      "learning_rate": 3.9124579124579127e-07,
      "loss": 0.568,
      "step": 581
    },
    {
      "epoch": 0.005878787878787879,
      "grad_norm": 0.35653141140937805,
      "learning_rate": 3.9191919191919196e-07,
      "loss": 1.3635,
      "step": 582
    },
    {
      "epoch": 0.005888888888888889,
      "grad_norm": 0.11830278486013412,
      "learning_rate": 3.925925925925926e-07,
      "loss": 0.5781,
      "step": 583
    },
    {
      "epoch": 0.005898989898989899,
      "grad_norm": 0.19566336274147034,
      "learning_rate": 3.9326599326599333e-07,
      "loss": 0.6756,
      "step": 584
    },
    {
      "epoch": 0.005909090909090909,
      "grad_norm": 0.11422201991081238,
      "learning_rate": 3.9393939393939396e-07,
      "loss": 0.5954,
      "step": 585
    },
    {
      "epoch": 0.0059191919191919195,
      "grad_norm": 0.19946463406085968,
      "learning_rate": 3.9461279461279465e-07,
      "loss": 0.6103,
      "step": 586
    },
    {
      "epoch": 0.005929292929292929,
      "grad_norm": 0.15899081528186798,
      "learning_rate": 3.9528619528619534e-07,
      "loss": 0.4425,
      "step": 587
    },
    {
      "epoch": 0.00593939393939394,
      "grad_norm": 0.09600074589252472,
      "learning_rate": 3.9595959595959597e-07,
      "loss": 0.5533,
      "step": 588
    },
    {
      "epoch": 0.005949494949494949,
      "grad_norm": 0.18410786986351013,
      "learning_rate": 3.9663299663299666e-07,
      "loss": 0.7572,
      "step": 589
    },
    {
      "epoch": 0.00595959595959596,
      "grad_norm": 0.09730228781700134,
      "learning_rate": 3.973063973063973e-07,
      "loss": 0.6642,
      "step": 590
    },
    {
      "epoch": 0.0059696969696969695,
      "grad_norm": 0.1130283921957016,
      "learning_rate": 3.9797979797979803e-07,
      "loss": 0.5527,
      "step": 591
    },
    {
      "epoch": 0.00597979797979798,
      "grad_norm": 0.15356935560703278,
      "learning_rate": 3.986531986531987e-07,
      "loss": 0.6818,
      "step": 592
    },
    {
      "epoch": 0.00598989898989899,
      "grad_norm": 0.16189977526664734,
      "learning_rate": 3.9932659932659935e-07,
      "loss": 1.2946,
      "step": 593
    },
    {
      "epoch": 0.006,
      "grad_norm": 0.12849482893943787,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.7347,
      "step": 594
    },
    {
      "epoch": 0.00601010101010101,
      "grad_norm": 0.15899981558322906,
      "learning_rate": 4.0067340067340067e-07,
      "loss": 0.7716,
      "step": 595
    },
    {
      "epoch": 0.00602020202020202,
      "grad_norm": 0.13782435655593872,
      "learning_rate": 4.0134680134680135e-07,
      "loss": 0.6691,
      "step": 596
    },
    {
      "epoch": 0.00603030303030303,
      "grad_norm": 0.15655741095542908,
      "learning_rate": 4.020202020202021e-07,
      "loss": 0.9326,
      "step": 597
    },
    {
      "epoch": 0.0060404040404040404,
      "grad_norm": 0.1434907466173172,
      "learning_rate": 4.0269360269360273e-07,
      "loss": 0.3575,
      "step": 598
    },
    {
      "epoch": 0.00605050505050505,
      "grad_norm": 0.11314640939235687,
      "learning_rate": 4.033670033670034e-07,
      "loss": 0.6644,
      "step": 599
    },
    {
      "epoch": 0.006060606060606061,
      "grad_norm": 0.1604374498128891,
      "learning_rate": 4.040404040404041e-07,
      "loss": 0.3597,
      "step": 600
    },
    {
      "epoch": 0.006070707070707071,
      "grad_norm": 0.09356118738651276,
      "learning_rate": 4.0471380471380473e-07,
      "loss": 0.5263,
      "step": 601
    },
    {
      "epoch": 0.006080808080808081,
      "grad_norm": 0.11354134231805801,
      "learning_rate": 4.053872053872054e-07,
      "loss": 0.5931,
      "step": 602
    },
    {
      "epoch": 0.006090909090909091,
      "grad_norm": 0.16742990911006927,
      "learning_rate": 4.0606060606060605e-07,
      "loss": 0.6175,
      "step": 603
    },
    {
      "epoch": 0.006101010101010101,
      "grad_norm": 0.10578832030296326,
      "learning_rate": 4.067340067340068e-07,
      "loss": 0.7189,
      "step": 604
    },
    {
      "epoch": 0.006111111111111111,
      "grad_norm": 0.17509733140468597,
      "learning_rate": 4.074074074074075e-07,
      "loss": 1.0326,
      "step": 605
    },
    {
      "epoch": 0.006121212121212121,
      "grad_norm": 0.13425634801387787,
      "learning_rate": 4.080808080808081e-07,
      "loss": 0.6131,
      "step": 606
    },
    {
      "epoch": 0.006131313131313132,
      "grad_norm": 0.21424785256385803,
      "learning_rate": 4.087542087542088e-07,
      "loss": 0.5819,
      "step": 607
    },
    {
      "epoch": 0.006141414141414141,
      "grad_norm": 0.1572229266166687,
      "learning_rate": 4.0942760942760943e-07,
      "loss": 0.5195,
      "step": 608
    },
    {
      "epoch": 0.006151515151515152,
      "grad_norm": 0.157199889421463,
      "learning_rate": 4.101010101010101e-07,
      "loss": 0.6881,
      "step": 609
    },
    {
      "epoch": 0.006161616161616161,
      "grad_norm": 0.1120857372879982,
      "learning_rate": 4.1077441077441086e-07,
      "loss": 0.504,
      "step": 610
    },
    {
      "epoch": 0.006171717171717172,
      "grad_norm": 0.11600152403116226,
      "learning_rate": 4.114478114478115e-07,
      "loss": 0.5797,
      "step": 611
    },
    {
      "epoch": 0.0061818181818181816,
      "grad_norm": 0.1403857171535492,
      "learning_rate": 4.121212121212122e-07,
      "loss": 0.3927,
      "step": 612
    },
    {
      "epoch": 0.006191919191919192,
      "grad_norm": 0.2880555987358093,
      "learning_rate": 4.127946127946128e-07,
      "loss": 0.946,
      "step": 613
    },
    {
      "epoch": 0.006202020202020202,
      "grad_norm": 0.4630388617515564,
      "learning_rate": 4.134680134680135e-07,
      "loss": 1.0003,
      "step": 614
    },
    {
      "epoch": 0.006212121212121212,
      "grad_norm": 0.1177152469754219,
      "learning_rate": 4.141414141414142e-07,
      "loss": 0.6083,
      "step": 615
    },
    {
      "epoch": 0.006222222222222222,
      "grad_norm": 0.18474844098091125,
      "learning_rate": 4.148148148148148e-07,
      "loss": 0.5924,
      "step": 616
    },
    {
      "epoch": 0.006232323232323232,
      "grad_norm": 0.2325655072927475,
      "learning_rate": 4.1548821548821556e-07,
      "loss": 0.4883,
      "step": 617
    },
    {
      "epoch": 0.006242424242424242,
      "grad_norm": 0.18236668407917023,
      "learning_rate": 4.161616161616162e-07,
      "loss": 0.9891,
      "step": 618
    },
    {
      "epoch": 0.0062525252525252525,
      "grad_norm": 0.22107923030853271,
      "learning_rate": 4.168350168350169e-07,
      "loss": 1.2609,
      "step": 619
    },
    {
      "epoch": 0.006262626262626263,
      "grad_norm": 0.12253084778785706,
      "learning_rate": 4.1750841750841756e-07,
      "loss": 0.6187,
      "step": 620
    },
    {
      "epoch": 0.006272727272727273,
      "grad_norm": 0.19084228575229645,
      "learning_rate": 4.181818181818182e-07,
      "loss": 0.8431,
      "step": 621
    },
    {
      "epoch": 0.006282828282828283,
      "grad_norm": 0.11282113939523697,
      "learning_rate": 4.188552188552189e-07,
      "loss": 0.6379,
      "step": 622
    },
    {
      "epoch": 0.006292929292929293,
      "grad_norm": 0.11325237154960632,
      "learning_rate": 4.195286195286195e-07,
      "loss": 0.4773,
      "step": 623
    },
    {
      "epoch": 0.006303030303030303,
      "grad_norm": 0.15797702968120575,
      "learning_rate": 4.2020202020202026e-07,
      "loss": 0.3356,
      "step": 624
    },
    {
      "epoch": 0.006313131313131313,
      "grad_norm": 0.20098933577537537,
      "learning_rate": 4.2087542087542094e-07,
      "loss": 0.3605,
      "step": 625
    },
    {
      "epoch": 0.0063232323232323235,
      "grad_norm": 0.14176684617996216,
      "learning_rate": 4.215488215488216e-07,
      "loss": 0.8603,
      "step": 626
    },
    {
      "epoch": 0.006333333333333333,
      "grad_norm": 0.2788138687610626,
      "learning_rate": 4.2222222222222226e-07,
      "loss": 0.9105,
      "step": 627
    },
    {
      "epoch": 0.006343434343434344,
      "grad_norm": 0.13549882173538208,
      "learning_rate": 4.228956228956229e-07,
      "loss": 0.46,
      "step": 628
    },
    {
      "epoch": 0.006353535353535353,
      "grad_norm": 0.27553385496139526,
      "learning_rate": 4.235690235690236e-07,
      "loss": 0.664,
      "step": 629
    },
    {
      "epoch": 0.006363636363636364,
      "grad_norm": 0.3083318769931793,
      "learning_rate": 4.242424242424243e-07,
      "loss": 0.8244,
      "step": 630
    },
    {
      "epoch": 0.0063737373737373735,
      "grad_norm": 0.15007786452770233,
      "learning_rate": 4.2491582491582495e-07,
      "loss": 0.6236,
      "step": 631
    },
    {
      "epoch": 0.006383838383838384,
      "grad_norm": 0.2027968019247055,
      "learning_rate": 4.2558922558922564e-07,
      "loss": 0.9747,
      "step": 632
    },
    {
      "epoch": 0.006393939393939394,
      "grad_norm": 0.18641731142997742,
      "learning_rate": 4.262626262626263e-07,
      "loss": 0.834,
      "step": 633
    },
    {
      "epoch": 0.006404040404040404,
      "grad_norm": 0.22143416106700897,
      "learning_rate": 4.2693602693602696e-07,
      "loss": 0.7055,
      "step": 634
    },
    {
      "epoch": 0.006414141414141414,
      "grad_norm": 0.08558551222085953,
      "learning_rate": 4.2760942760942765e-07,
      "loss": 0.6958,
      "step": 635
    },
    {
      "epoch": 0.006424242424242424,
      "grad_norm": 0.09387972205877304,
      "learning_rate": 4.282828282828283e-07,
      "loss": 0.6012,
      "step": 636
    },
    {
      "epoch": 0.006434343434343434,
      "grad_norm": 0.11976263672113419,
      "learning_rate": 4.28956228956229e-07,
      "loss": 0.5265,
      "step": 637
    },
    {
      "epoch": 0.0064444444444444445,
      "grad_norm": 0.16035473346710205,
      "learning_rate": 4.2962962962962965e-07,
      "loss": 0.712,
      "step": 638
    },
    {
      "epoch": 0.006454545454545454,
      "grad_norm": 0.23505857586860657,
      "learning_rate": 4.3030303030303034e-07,
      "loss": 0.76,
      "step": 639
    },
    {
      "epoch": 0.006464646464646465,
      "grad_norm": 0.18381190299987793,
      "learning_rate": 4.30976430976431e-07,
      "loss": 0.5464,
      "step": 640
    },
    {
      "epoch": 0.006474747474747475,
      "grad_norm": 0.11085553467273712,
      "learning_rate": 4.3164983164983166e-07,
      "loss": 0.5816,
      "step": 641
    },
    {
      "epoch": 0.006484848484848485,
      "grad_norm": 0.2560874819755554,
      "learning_rate": 4.3232323232323235e-07,
      "loss": 0.7054,
      "step": 642
    },
    {
      "epoch": 0.006494949494949495,
      "grad_norm": 0.2291979193687439,
      "learning_rate": 4.32996632996633e-07,
      "loss": 0.4686,
      "step": 643
    },
    {
      "epoch": 0.006505050505050505,
      "grad_norm": 0.13927139341831207,
      "learning_rate": 4.336700336700337e-07,
      "loss": 0.5802,
      "step": 644
    },
    {
      "epoch": 0.0065151515151515155,
      "grad_norm": 0.11879061162471771,
      "learning_rate": 4.343434343434344e-07,
      "loss": 1.3873,
      "step": 645
    },
    {
      "epoch": 0.006525252525252525,
      "grad_norm": 0.10775404423475266,
      "learning_rate": 4.3501683501683504e-07,
      "loss": 0.5827,
      "step": 646
    },
    {
      "epoch": 0.006535353535353536,
      "grad_norm": 0.11822811514139175,
      "learning_rate": 4.356902356902357e-07,
      "loss": 0.8715,
      "step": 647
    },
    {
      "epoch": 0.006545454545454545,
      "grad_norm": 0.16305166482925415,
      "learning_rate": 4.363636363636364e-07,
      "loss": 0.6304,
      "step": 648
    },
    {
      "epoch": 0.006555555555555556,
      "grad_norm": 0.1420610398054123,
      "learning_rate": 4.3703703703703704e-07,
      "loss": 0.7268,
      "step": 649
    },
    {
      "epoch": 0.0065656565656565654,
      "grad_norm": 0.1567188799381256,
      "learning_rate": 4.377104377104378e-07,
      "loss": 0.4763,
      "step": 650
    },
    {
      "epoch": 0.006575757575757576,
      "grad_norm": 0.19705195724964142,
      "learning_rate": 4.383838383838384e-07,
      "loss": 0.6398,
      "step": 651
    },
    {
      "epoch": 0.006585858585858586,
      "grad_norm": 0.15087905526161194,
      "learning_rate": 4.390572390572391e-07,
      "loss": 0.3207,
      "step": 652
    },
    {
      "epoch": 0.006595959595959596,
      "grad_norm": 0.09464576840400696,
      "learning_rate": 4.397306397306398e-07,
      "loss": 0.6902,
      "step": 653
    },
    {
      "epoch": 0.006606060606060606,
      "grad_norm": 0.17911985516548157,
      "learning_rate": 4.404040404040404e-07,
      "loss": 0.3436,
      "step": 654
    },
    {
      "epoch": 0.006616161616161616,
      "grad_norm": 0.18400679528713226,
      "learning_rate": 4.410774410774411e-07,
      "loss": 1.0408,
      "step": 655
    },
    {
      "epoch": 0.006626262626262626,
      "grad_norm": 0.15315650403499603,
      "learning_rate": 4.4175084175084174e-07,
      "loss": 0.4423,
      "step": 656
    },
    {
      "epoch": 0.006636363636363636,
      "grad_norm": 0.2583278715610504,
      "learning_rate": 4.424242424242425e-07,
      "loss": 0.6577,
      "step": 657
    },
    {
      "epoch": 0.006646464646464646,
      "grad_norm": 0.09262445569038391,
      "learning_rate": 4.4309764309764317e-07,
      "loss": 0.5618,
      "step": 658
    },
    {
      "epoch": 0.006656565656565657,
      "grad_norm": 0.21606825292110443,
      "learning_rate": 4.437710437710438e-07,
      "loss": 0.4343,
      "step": 659
    },
    {
      "epoch": 0.006666666666666667,
      "grad_norm": 0.17032340168952942,
      "learning_rate": 4.444444444444445e-07,
      "loss": 0.4009,
      "step": 660
    },
    {
      "epoch": 0.006676767676767677,
      "grad_norm": 0.14078263938426971,
      "learning_rate": 4.451178451178451e-07,
      "loss": 0.6907,
      "step": 661
    },
    {
      "epoch": 0.006686868686868687,
      "grad_norm": 0.2794434726238251,
      "learning_rate": 4.457912457912458e-07,
      "loss": 0.9051,
      "step": 662
    },
    {
      "epoch": 0.006696969696969697,
      "grad_norm": 0.26195138692855835,
      "learning_rate": 4.4646464646464655e-07,
      "loss": 0.6995,
      "step": 663
    },
    {
      "epoch": 0.006707070707070707,
      "grad_norm": 0.11013339459896088,
      "learning_rate": 4.471380471380472e-07,
      "loss": 0.5839,
      "step": 664
    },
    {
      "epoch": 0.006717171717171717,
      "grad_norm": 0.11240403354167938,
      "learning_rate": 4.4781144781144787e-07,
      "loss": 0.5937,
      "step": 665
    },
    {
      "epoch": 0.006727272727272728,
      "grad_norm": 0.1347968727350235,
      "learning_rate": 4.484848484848485e-07,
      "loss": 0.9229,
      "step": 666
    },
    {
      "epoch": 0.006737373737373737,
      "grad_norm": 0.17129011452198029,
      "learning_rate": 4.491582491582492e-07,
      "loss": 0.6806,
      "step": 667
    },
    {
      "epoch": 0.006747474747474748,
      "grad_norm": 0.22120998799800873,
      "learning_rate": 4.498316498316499e-07,
      "loss": 0.6405,
      "step": 668
    },
    {
      "epoch": 0.006757575757575757,
      "grad_norm": 0.14746014773845673,
      "learning_rate": 4.505050505050505e-07,
      "loss": 0.5542,
      "step": 669
    },
    {
      "epoch": 0.006767676767676768,
      "grad_norm": 0.13065361976623535,
      "learning_rate": 4.5117845117845125e-07,
      "loss": 0.5173,
      "step": 670
    },
    {
      "epoch": 0.0067777777777777775,
      "grad_norm": 0.11704938858747482,
      "learning_rate": 4.518518518518519e-07,
      "loss": 0.8415,
      "step": 671
    },
    {
      "epoch": 0.006787878787878788,
      "grad_norm": 0.10242617130279541,
      "learning_rate": 4.5252525252525257e-07,
      "loss": 0.4477,
      "step": 672
    },
    {
      "epoch": 0.006797979797979798,
      "grad_norm": 0.19505098462104797,
      "learning_rate": 4.5319865319865325e-07,
      "loss": 0.6012,
      "step": 673
    },
    {
      "epoch": 0.006808080808080808,
      "grad_norm": 0.18871420621871948,
      "learning_rate": 4.538720538720539e-07,
      "loss": 0.6803,
      "step": 674
    },
    {
      "epoch": 0.006818181818181818,
      "grad_norm": 0.1341879516839981,
      "learning_rate": 4.5454545454545457e-07,
      "loss": 0.3575,
      "step": 675
    },
    {
      "epoch": 0.006828282828282828,
      "grad_norm": 0.14173880219459534,
      "learning_rate": 4.552188552188552e-07,
      "loss": 0.5295,
      "step": 676
    },
    {
      "epoch": 0.006838383838383838,
      "grad_norm": 0.12866951525211334,
      "learning_rate": 4.5589225589225595e-07,
      "loss": 0.4259,
      "step": 677
    },
    {
      "epoch": 0.0068484848484848485,
      "grad_norm": 0.16827286779880524,
      "learning_rate": 4.5656565656565663e-07,
      "loss": 0.6476,
      "step": 678
    },
    {
      "epoch": 0.006858585858585858,
      "grad_norm": 0.14215239882469177,
      "learning_rate": 4.5723905723905727e-07,
      "loss": 0.6517,
      "step": 679
    },
    {
      "epoch": 0.006868686868686869,
      "grad_norm": 0.13707281649112701,
      "learning_rate": 4.5791245791245795e-07,
      "loss": 0.4554,
      "step": 680
    },
    {
      "epoch": 0.006878787878787879,
      "grad_norm": 0.1189674586057663,
      "learning_rate": 4.585858585858586e-07,
      "loss": 0.7317,
      "step": 681
    },
    {
      "epoch": 0.006888888888888889,
      "grad_norm": 0.08771689236164093,
      "learning_rate": 4.5925925925925927e-07,
      "loss": 0.7555,
      "step": 682
    },
    {
      "epoch": 0.006898989898989899,
      "grad_norm": 0.18629240989685059,
      "learning_rate": 4.5993265993266e-07,
      "loss": 0.9398,
      "step": 683
    },
    {
      "epoch": 0.006909090909090909,
      "grad_norm": 0.3186473250389099,
      "learning_rate": 4.6060606060606064e-07,
      "loss": 0.7068,
      "step": 684
    },
    {
      "epoch": 0.0069191919191919195,
      "grad_norm": 0.15602163970470428,
      "learning_rate": 4.6127946127946133e-07,
      "loss": 0.7403,
      "step": 685
    },
    {
      "epoch": 0.006929292929292929,
      "grad_norm": 0.17623919248580933,
      "learning_rate": 4.6195286195286196e-07,
      "loss": 0.5015,
      "step": 686
    },
    {
      "epoch": 0.00693939393939394,
      "grad_norm": 0.16385093331336975,
      "learning_rate": 4.6262626262626265e-07,
      "loss": 0.8185,
      "step": 687
    },
    {
      "epoch": 0.006949494949494949,
      "grad_norm": 0.11162284761667252,
      "learning_rate": 4.6329966329966334e-07,
      "loss": 0.6312,
      "step": 688
    },
    {
      "epoch": 0.00695959595959596,
      "grad_norm": 0.21831049025058746,
      "learning_rate": 4.6397306397306397e-07,
      "loss": 0.3366,
      "step": 689
    },
    {
      "epoch": 0.0069696969696969695,
      "grad_norm": 0.14694111049175262,
      "learning_rate": 4.646464646464647e-07,
      "loss": 0.4339,
      "step": 690
    },
    {
      "epoch": 0.00697979797979798,
      "grad_norm": 0.16253265738487244,
      "learning_rate": 4.6531986531986534e-07,
      "loss": 0.4033,
      "step": 691
    },
    {
      "epoch": 0.00698989898989899,
      "grad_norm": 0.12993334233760834,
      "learning_rate": 4.6599326599326603e-07,
      "loss": 0.5465,
      "step": 692
    },
    {
      "epoch": 0.007,
      "grad_norm": 0.14917369186878204,
      "learning_rate": 4.666666666666667e-07,
      "loss": 0.6307,
      "step": 693
    },
    {
      "epoch": 0.00701010101010101,
      "grad_norm": 0.16724835336208344,
      "learning_rate": 4.6734006734006735e-07,
      "loss": 1.0032,
      "step": 694
    },
    {
      "epoch": 0.00702020202020202,
      "grad_norm": 0.17822004854679108,
      "learning_rate": 4.6801346801346804e-07,
      "loss": 0.788,
      "step": 695
    },
    {
      "epoch": 0.00703030303030303,
      "grad_norm": 0.1848423182964325,
      "learning_rate": 4.686868686868688e-07,
      "loss": 1.1094,
      "step": 696
    },
    {
      "epoch": 0.0070404040404040405,
      "grad_norm": 0.10623061656951904,
      "learning_rate": 4.693602693602694e-07,
      "loss": 0.5713,
      "step": 697
    },
    {
      "epoch": 0.00705050505050505,
      "grad_norm": 0.2684580087661743,
      "learning_rate": 4.700336700336701e-07,
      "loss": 0.8627,
      "step": 698
    },
    {
      "epoch": 0.007060606060606061,
      "grad_norm": 0.17375481128692627,
      "learning_rate": 4.7070707070707073e-07,
      "loss": 0.345,
      "step": 699
    },
    {
      "epoch": 0.007070707070707071,
      "grad_norm": 0.14827989041805267,
      "learning_rate": 4.713804713804714e-07,
      "loss": 0.3797,
      "step": 700
    },
    {
      "epoch": 0.007080808080808081,
      "grad_norm": 0.13792623579502106,
      "learning_rate": 4.720538720538721e-07,
      "loss": 0.62,
      "step": 701
    },
    {
      "epoch": 0.007090909090909091,
      "grad_norm": 0.10292626917362213,
      "learning_rate": 4.7272727272727273e-07,
      "loss": 0.3654,
      "step": 702
    },
    {
      "epoch": 0.007101010101010101,
      "grad_norm": 0.12607921659946442,
      "learning_rate": 4.734006734006735e-07,
      "loss": 0.6973,
      "step": 703
    },
    {
      "epoch": 0.0071111111111111115,
      "grad_norm": 0.2708573043346405,
      "learning_rate": 4.740740740740741e-07,
      "loss": 0.9622,
      "step": 704
    },
    {
      "epoch": 0.007121212121212121,
      "grad_norm": 0.11547540873289108,
      "learning_rate": 4.747474747474748e-07,
      "loss": 0.6159,
      "step": 705
    },
    {
      "epoch": 0.007131313131313132,
      "grad_norm": 0.224186971783638,
      "learning_rate": 4.754208754208755e-07,
      "loss": 0.8902,
      "step": 706
    },
    {
      "epoch": 0.007141414141414141,
      "grad_norm": 0.12676557898521423,
      "learning_rate": 4.760942760942761e-07,
      "loss": 0.7294,
      "step": 707
    },
    {
      "epoch": 0.007151515151515152,
      "grad_norm": 0.3761444687843323,
      "learning_rate": 4.767676767676768e-07,
      "loss": 0.7758,
      "step": 708
    },
    {
      "epoch": 0.007161616161616161,
      "grad_norm": 0.11560038477182388,
      "learning_rate": 4.774410774410775e-07,
      "loss": 0.3245,
      "step": 709
    },
    {
      "epoch": 0.007171717171717172,
      "grad_norm": 0.19183412194252014,
      "learning_rate": 4.781144781144781e-07,
      "loss": 0.4435,
      "step": 710
    },
    {
      "epoch": 0.007181818181818182,
      "grad_norm": 0.3264574408531189,
      "learning_rate": 4.787878787878789e-07,
      "loss": 0.7952,
      "step": 711
    },
    {
      "epoch": 0.007191919191919192,
      "grad_norm": 0.1057809442281723,
      "learning_rate": 4.794612794612795e-07,
      "loss": 0.8394,
      "step": 712
    },
    {
      "epoch": 0.007202020202020202,
      "grad_norm": 0.2214202880859375,
      "learning_rate": 4.801346801346802e-07,
      "loss": 0.6909,
      "step": 713
    },
    {
      "epoch": 0.007212121212121212,
      "grad_norm": 0.12135957181453705,
      "learning_rate": 4.808080808080809e-07,
      "loss": 0.8923,
      "step": 714
    },
    {
      "epoch": 0.007222222222222222,
      "grad_norm": 0.13880972564220428,
      "learning_rate": 4.814814814814815e-07,
      "loss": 0.47,
      "step": 715
    },
    {
      "epoch": 0.007232323232323232,
      "grad_norm": 0.11161673069000244,
      "learning_rate": 4.821548821548822e-07,
      "loss": 0.5657,
      "step": 716
    },
    {
      "epoch": 0.007242424242424242,
      "grad_norm": 0.16069956123828888,
      "learning_rate": 4.828282828282829e-07,
      "loss": 0.7039,
      "step": 717
    },
    {
      "epoch": 0.0072525252525252526,
      "grad_norm": 0.2107360064983368,
      "learning_rate": 4.835016835016835e-07,
      "loss": 0.7521,
      "step": 718
    },
    {
      "epoch": 0.007262626262626262,
      "grad_norm": 0.18116039037704468,
      "learning_rate": 4.841750841750841e-07,
      "loss": 0.97,
      "step": 719
    },
    {
      "epoch": 0.007272727272727273,
      "grad_norm": 0.13099129498004913,
      "learning_rate": 4.848484848484849e-07,
      "loss": 0.6402,
      "step": 720
    },
    {
      "epoch": 0.007282828282828283,
      "grad_norm": 0.13760921359062195,
      "learning_rate": 4.855218855218856e-07,
      "loss": 0.6193,
      "step": 721
    },
    {
      "epoch": 0.007292929292929293,
      "grad_norm": 0.13530674576759338,
      "learning_rate": 4.861952861952863e-07,
      "loss": 0.6844,
      "step": 722
    },
    {
      "epoch": 0.007303030303030303,
      "grad_norm": 0.21845480799674988,
      "learning_rate": 4.868686868686869e-07,
      "loss": 0.9538,
      "step": 723
    },
    {
      "epoch": 0.007313131313131313,
      "grad_norm": 0.14229154586791992,
      "learning_rate": 4.875420875420875e-07,
      "loss": 0.87,
      "step": 724
    },
    {
      "epoch": 0.0073232323232323236,
      "grad_norm": 0.1097828820347786,
      "learning_rate": 4.882154882154883e-07,
      "loss": 0.4004,
      "step": 725
    },
    {
      "epoch": 0.007333333333333333,
      "grad_norm": 0.17427068948745728,
      "learning_rate": 4.88888888888889e-07,
      "loss": 0.5988,
      "step": 726
    },
    {
      "epoch": 0.007343434343434344,
      "grad_norm": 0.13226577639579773,
      "learning_rate": 4.895622895622896e-07,
      "loss": 0.5652,
      "step": 727
    },
    {
      "epoch": 0.007353535353535353,
      "grad_norm": 0.11013868451118469,
      "learning_rate": 4.902356902356903e-07,
      "loss": 0.5875,
      "step": 728
    },
    {
      "epoch": 0.007363636363636364,
      "grad_norm": 0.09546580165624619,
      "learning_rate": 4.909090909090909e-07,
      "loss": 0.6472,
      "step": 729
    },
    {
      "epoch": 0.0073737373737373735,
      "grad_norm": 0.1547175496816635,
      "learning_rate": 4.915824915824916e-07,
      "loss": 0.3552,
      "step": 730
    },
    {
      "epoch": 0.007383838383838384,
      "grad_norm": 0.10502071678638458,
      "learning_rate": 4.922558922558923e-07,
      "loss": 0.528,
      "step": 731
    },
    {
      "epoch": 0.007393939393939394,
      "grad_norm": 0.0906011164188385,
      "learning_rate": 4.929292929292929e-07,
      "loss": 0.8271,
      "step": 732
    },
    {
      "epoch": 0.007404040404040404,
      "grad_norm": 0.09122262895107269,
      "learning_rate": 4.936026936026936e-07,
      "loss": 0.681,
      "step": 733
    },
    {
      "epoch": 0.007414141414141414,
      "grad_norm": 0.14083606004714966,
      "learning_rate": 4.942760942760943e-07,
      "loss": 0.2992,
      "step": 734
    },
    {
      "epoch": 0.007424242424242424,
      "grad_norm": 0.22136186063289642,
      "learning_rate": 4.94949494949495e-07,
      "loss": 0.8358,
      "step": 735
    },
    {
      "epoch": 0.007434343434343434,
      "grad_norm": 0.1111910343170166,
      "learning_rate": 4.956228956228956e-07,
      "loss": 0.8505,
      "step": 736
    },
    {
      "epoch": 0.0074444444444444445,
      "grad_norm": 0.09917082637548447,
      "learning_rate": 4.962962962962963e-07,
      "loss": 0.6832,
      "step": 737
    },
    {
      "epoch": 0.007454545454545454,
      "grad_norm": 0.1172311007976532,
      "learning_rate": 4.96969696969697e-07,
      "loss": 0.8192,
      "step": 738
    },
    {
      "epoch": 0.007464646464646465,
      "grad_norm": 0.21497447788715363,
      "learning_rate": 4.976430976430977e-07,
      "loss": 0.4787,
      "step": 739
    },
    {
      "epoch": 0.007474747474747475,
      "grad_norm": 0.22201204299926758,
      "learning_rate": 4.983164983164984e-07,
      "loss": 0.7373,
      "step": 740
    },
    {
      "epoch": 0.007484848484848485,
      "grad_norm": 0.17775742709636688,
      "learning_rate": 4.98989898989899e-07,
      "loss": 0.4103,
      "step": 741
    },
    {
      "epoch": 0.007494949494949495,
      "grad_norm": 0.11624675244092941,
      "learning_rate": 4.996632996632997e-07,
      "loss": 0.6526,
      "step": 742
    },
    {
      "epoch": 0.007505050505050505,
      "grad_norm": 0.17156490683555603,
      "learning_rate": 5.003367003367004e-07,
      "loss": 0.4799,
      "step": 743
    },
    {
      "epoch": 0.0075151515151515155,
      "grad_norm": 0.22007542848587036,
      "learning_rate": 5.01010101010101e-07,
      "loss": 1.1497,
      "step": 744
    },
    {
      "epoch": 0.007525252525252525,
      "grad_norm": 0.17442521452903748,
      "learning_rate": 5.016835016835017e-07,
      "loss": 0.3229,
      "step": 745
    },
    {
      "epoch": 0.007535353535353536,
      "grad_norm": 0.17304982244968414,
      "learning_rate": 5.023569023569024e-07,
      "loss": 0.9732,
      "step": 746
    },
    {
      "epoch": 0.007545454545454545,
      "grad_norm": 0.16082479059696198,
      "learning_rate": 5.03030303030303e-07,
      "loss": 0.7205,
      "step": 747
    },
    {
      "epoch": 0.007555555555555556,
      "grad_norm": 0.2516406774520874,
      "learning_rate": 5.037037037037038e-07,
      "loss": 0.5496,
      "step": 748
    },
    {
      "epoch": 0.0075656565656565655,
      "grad_norm": 0.13451622426509857,
      "learning_rate": 5.043771043771044e-07,
      "loss": 0.4121,
      "step": 749
    },
    {
      "epoch": 0.007575757575757576,
      "grad_norm": 0.13422100245952606,
      "learning_rate": 5.05050505050505e-07,
      "loss": 0.5763,
      "step": 750
    },
    {
      "epoch": 0.007585858585858586,
      "grad_norm": 0.15408706665039062,
      "learning_rate": 5.057239057239058e-07,
      "loss": 0.7207,
      "step": 751
    },
    {
      "epoch": 0.007595959595959596,
      "grad_norm": 0.1636921763420105,
      "learning_rate": 5.063973063973064e-07,
      "loss": 0.4928,
      "step": 752
    },
    {
      "epoch": 0.007606060606060606,
      "grad_norm": 0.14278635382652283,
      "learning_rate": 5.070707070707072e-07,
      "loss": 0.547,
      "step": 753
    },
    {
      "epoch": 0.007616161616161616,
      "grad_norm": 0.16411279141902924,
      "learning_rate": 5.077441077441078e-07,
      "loss": 0.6912,
      "step": 754
    },
    {
      "epoch": 0.007626262626262626,
      "grad_norm": 0.12237254530191422,
      "learning_rate": 5.084175084175084e-07,
      "loss": 0.8071,
      "step": 755
    },
    {
      "epoch": 0.0076363636363636364,
      "grad_norm": 0.10057274997234344,
      "learning_rate": 5.090909090909092e-07,
      "loss": 0.5167,
      "step": 756
    },
    {
      "epoch": 0.007646464646464646,
      "grad_norm": 0.17265865206718445,
      "learning_rate": 5.097643097643098e-07,
      "loss": 0.4369,
      "step": 757
    },
    {
      "epoch": 0.007656565656565657,
      "grad_norm": 0.08709356188774109,
      "learning_rate": 5.104377104377104e-07,
      "loss": 0.5985,
      "step": 758
    },
    {
      "epoch": 0.007666666666666666,
      "grad_norm": 0.13611453771591187,
      "learning_rate": 5.111111111111112e-07,
      "loss": 0.4588,
      "step": 759
    },
    {
      "epoch": 0.007676767676767677,
      "grad_norm": 0.20197799801826477,
      "learning_rate": 5.117845117845118e-07,
      "loss": 0.959,
      "step": 760
    },
    {
      "epoch": 0.007686868686868687,
      "grad_norm": 0.14411519467830658,
      "learning_rate": 5.124579124579125e-07,
      "loss": 0.6666,
      "step": 761
    },
    {
      "epoch": 0.007696969696969697,
      "grad_norm": 0.2050352543592453,
      "learning_rate": 5.131313131313132e-07,
      "loss": 0.9912,
      "step": 762
    },
    {
      "epoch": 0.007707070707070707,
      "grad_norm": 0.2406359761953354,
      "learning_rate": 5.138047138047138e-07,
      "loss": 1.0676,
      "step": 763
    },
    {
      "epoch": 0.007717171717171717,
      "grad_norm": 0.1481846570968628,
      "learning_rate": 5.144781144781145e-07,
      "loss": 0.4252,
      "step": 764
    },
    {
      "epoch": 0.007727272727272728,
      "grad_norm": 0.1502930372953415,
      "learning_rate": 5.151515151515152e-07,
      "loss": 0.5956,
      "step": 765
    },
    {
      "epoch": 0.007737373737373737,
      "grad_norm": 0.203053280711174,
      "learning_rate": 5.158249158249159e-07,
      "loss": 1.0896,
      "step": 766
    },
    {
      "epoch": 0.007747474747474748,
      "grad_norm": 0.4027634859085083,
      "learning_rate": 5.164983164983166e-07,
      "loss": 0.7461,
      "step": 767
    },
    {
      "epoch": 0.007757575757575757,
      "grad_norm": 0.13181999325752258,
      "learning_rate": 5.171717171717172e-07,
      "loss": 0.7249,
      "step": 768
    },
    {
      "epoch": 0.007767676767676768,
      "grad_norm": 0.1611776202917099,
      "learning_rate": 5.178451178451179e-07,
      "loss": 0.6042,
      "step": 769
    },
    {
      "epoch": 0.0077777777777777776,
      "grad_norm": 0.21484419703483582,
      "learning_rate": 5.185185185185186e-07,
      "loss": 0.8281,
      "step": 770
    },
    {
      "epoch": 0.007787878787878788,
      "grad_norm": 0.16146734356880188,
      "learning_rate": 5.191919191919192e-07,
      "loss": 0.3947,
      "step": 771
    },
    {
      "epoch": 0.007797979797979798,
      "grad_norm": 0.11911506205797195,
      "learning_rate": 5.198653198653198e-07,
      "loss": 0.741,
      "step": 772
    },
    {
      "epoch": 0.007808080808080808,
      "grad_norm": 0.16305096447467804,
      "learning_rate": 5.205387205387206e-07,
      "loss": 0.3393,
      "step": 773
    },
    {
      "epoch": 0.007818181818181818,
      "grad_norm": 0.3921135663986206,
      "learning_rate": 5.212121212121213e-07,
      "loss": 0.779,
      "step": 774
    },
    {
      "epoch": 0.007828282828282828,
      "grad_norm": 0.10489813983440399,
      "learning_rate": 5.218855218855219e-07,
      "loss": 0.5423,
      "step": 775
    },
    {
      "epoch": 0.007838383838383839,
      "grad_norm": 0.15342693030834198,
      "learning_rate": 5.225589225589226e-07,
      "loss": 0.8275,
      "step": 776
    },
    {
      "epoch": 0.007848484848484848,
      "grad_norm": 0.1839051991701126,
      "learning_rate": 5.232323232323232e-07,
      "loss": 0.5559,
      "step": 777
    },
    {
      "epoch": 0.007858585858585858,
      "grad_norm": 0.2021855264902115,
      "learning_rate": 5.239057239057239e-07,
      "loss": 0.8377,
      "step": 778
    },
    {
      "epoch": 0.007868686868686869,
      "grad_norm": 0.13663969933986664,
      "learning_rate": 5.245791245791247e-07,
      "loss": 0.5734,
      "step": 779
    },
    {
      "epoch": 0.00787878787878788,
      "grad_norm": 0.2141590714454651,
      "learning_rate": 5.252525252525253e-07,
      "loss": 0.7984,
      "step": 780
    },
    {
      "epoch": 0.00788888888888889,
      "grad_norm": 0.27831217646598816,
      "learning_rate": 5.25925925925926e-07,
      "loss": 0.806,
      "step": 781
    },
    {
      "epoch": 0.007898989898989899,
      "grad_norm": 0.32770469784736633,
      "learning_rate": 5.265993265993266e-07,
      "loss": 0.7732,
      "step": 782
    },
    {
      "epoch": 0.007909090909090909,
      "grad_norm": 0.2064051628112793,
      "learning_rate": 5.272727272727273e-07,
      "loss": 0.825,
      "step": 783
    },
    {
      "epoch": 0.00791919191919192,
      "grad_norm": 0.2050388902425766,
      "learning_rate": 5.27946127946128e-07,
      "loss": 0.4435,
      "step": 784
    },
    {
      "epoch": 0.00792929292929293,
      "grad_norm": 0.2135784775018692,
      "learning_rate": 5.286195286195286e-07,
      "loss": 0.956,
      "step": 785
    },
    {
      "epoch": 0.007939393939393939,
      "grad_norm": 0.11732328683137894,
      "learning_rate": 5.292929292929293e-07,
      "loss": 0.9043,
      "step": 786
    },
    {
      "epoch": 0.00794949494949495,
      "grad_norm": 0.15125510096549988,
      "learning_rate": 5.2996632996633e-07,
      "loss": 0.8114,
      "step": 787
    },
    {
      "epoch": 0.00795959595959596,
      "grad_norm": 0.13710205256938934,
      "learning_rate": 5.306397306397307e-07,
      "loss": 0.5515,
      "step": 788
    },
    {
      "epoch": 0.00796969696969697,
      "grad_norm": 0.19383318722248077,
      "learning_rate": 5.313131313131313e-07,
      "loss": 1.0808,
      "step": 789
    },
    {
      "epoch": 0.00797979797979798,
      "grad_norm": 0.17394188046455383,
      "learning_rate": 5.31986531986532e-07,
      "loss": 0.6757,
      "step": 790
    },
    {
      "epoch": 0.00798989898989899,
      "grad_norm": 0.18186749517917633,
      "learning_rate": 5.326599326599327e-07,
      "loss": 0.9125,
      "step": 791
    },
    {
      "epoch": 0.008,
      "grad_norm": 0.14665378630161285,
      "learning_rate": 5.333333333333335e-07,
      "loss": 0.4066,
      "step": 792
    },
    {
      "epoch": 0.00801010101010101,
      "grad_norm": 0.10829672962427139,
      "learning_rate": 5.340067340067341e-07,
      "loss": 0.5688,
      "step": 793
    },
    {
      "epoch": 0.00802020202020202,
      "grad_norm": 0.20901566743850708,
      "learning_rate": 5.346801346801347e-07,
      "loss": 1.1405,
      "step": 794
    },
    {
      "epoch": 0.00803030303030303,
      "grad_norm": 0.09858789294958115,
      "learning_rate": 5.353535353535354e-07,
      "loss": 0.5802,
      "step": 795
    },
    {
      "epoch": 0.00804040404040404,
      "grad_norm": 0.11997748911380768,
      "learning_rate": 5.360269360269361e-07,
      "loss": 0.5997,
      "step": 796
    },
    {
      "epoch": 0.008050505050505051,
      "grad_norm": 0.13188420236110687,
      "learning_rate": 5.367003367003368e-07,
      "loss": 0.7996,
      "step": 797
    },
    {
      "epoch": 0.00806060606060606,
      "grad_norm": 0.14726032316684723,
      "learning_rate": 5.373737373737374e-07,
      "loss": 0.8984,
      "step": 798
    },
    {
      "epoch": 0.00807070707070707,
      "grad_norm": 0.30528056621551514,
      "learning_rate": 5.380471380471381e-07,
      "loss": 0.9257,
      "step": 799
    },
    {
      "epoch": 0.00808080808080808,
      "grad_norm": 0.09363482147455215,
      "learning_rate": 5.387205387205387e-07,
      "loss": 0.3596,
      "step": 800
    },
    {
      "epoch": 0.008090909090909091,
      "grad_norm": 0.22678177058696747,
      "learning_rate": 5.393939393939395e-07,
      "loss": 0.4198,
      "step": 801
    },
    {
      "epoch": 0.008101010101010102,
      "grad_norm": 0.16721422970294952,
      "learning_rate": 5.400673400673401e-07,
      "loss": 0.5771,
      "step": 802
    },
    {
      "epoch": 0.00811111111111111,
      "grad_norm": 0.19358910620212555,
      "learning_rate": 5.407407407407407e-07,
      "loss": 0.5184,
      "step": 803
    },
    {
      "epoch": 0.008121212121212121,
      "grad_norm": 0.15558740496635437,
      "learning_rate": 5.414141414141415e-07,
      "loss": 0.5418,
      "step": 804
    },
    {
      "epoch": 0.008131313131313132,
      "grad_norm": 0.09712889045476913,
      "learning_rate": 5.420875420875421e-07,
      "loss": 0.9136,
      "step": 805
    },
    {
      "epoch": 0.008141414141414142,
      "grad_norm": 0.18942579627037048,
      "learning_rate": 5.427609427609428e-07,
      "loss": 0.3113,
      "step": 806
    },
    {
      "epoch": 0.008151515151515151,
      "grad_norm": 0.09377065300941467,
      "learning_rate": 5.434343434343435e-07,
      "loss": 0.6715,
      "step": 807
    },
    {
      "epoch": 0.008161616161616161,
      "grad_norm": 0.22203773260116577,
      "learning_rate": 5.441077441077441e-07,
      "loss": 0.7419,
      "step": 808
    },
    {
      "epoch": 0.008171717171717172,
      "grad_norm": 0.1456647515296936,
      "learning_rate": 5.447811447811449e-07,
      "loss": 0.5848,
      "step": 809
    },
    {
      "epoch": 0.008181818181818182,
      "grad_norm": 0.22210150957107544,
      "learning_rate": 5.454545454545455e-07,
      "loss": 0.8234,
      "step": 810
    },
    {
      "epoch": 0.008191919191919191,
      "grad_norm": 0.13140779733657837,
      "learning_rate": 5.461279461279461e-07,
      "loss": 0.8534,
      "step": 811
    },
    {
      "epoch": 0.008202020202020202,
      "grad_norm": 0.1735650599002838,
      "learning_rate": 5.468013468013469e-07,
      "loss": 0.6024,
      "step": 812
    },
    {
      "epoch": 0.008212121212121212,
      "grad_norm": 0.13748227059841156,
      "learning_rate": 5.474747474747475e-07,
      "loss": 0.646,
      "step": 813
    },
    {
      "epoch": 0.008222222222222223,
      "grad_norm": 0.24426423013210297,
      "learning_rate": 5.481481481481482e-07,
      "loss": 0.5696,
      "step": 814
    },
    {
      "epoch": 0.008232323232323232,
      "grad_norm": 0.14206145703792572,
      "learning_rate": 5.488215488215489e-07,
      "loss": 0.7555,
      "step": 815
    },
    {
      "epoch": 0.008242424242424242,
      "grad_norm": 0.18565820157527924,
      "learning_rate": 5.494949494949495e-07,
      "loss": 0.6963,
      "step": 816
    },
    {
      "epoch": 0.008252525252525253,
      "grad_norm": 0.17626863718032837,
      "learning_rate": 5.501683501683502e-07,
      "loss": 1.0327,
      "step": 817
    },
    {
      "epoch": 0.008262626262626263,
      "grad_norm": 0.12655490636825562,
      "learning_rate": 5.508417508417509e-07,
      "loss": 0.4668,
      "step": 818
    },
    {
      "epoch": 0.008272727272727272,
      "grad_norm": 0.10819879919290543,
      "learning_rate": 5.515151515151516e-07,
      "loss": 0.8532,
      "step": 819
    },
    {
      "epoch": 0.008282828282828282,
      "grad_norm": 0.1480506807565689,
      "learning_rate": 5.521885521885522e-07,
      "loss": 0.7134,
      "step": 820
    },
    {
      "epoch": 0.008292929292929293,
      "grad_norm": 0.2229781150817871,
      "learning_rate": 5.528619528619529e-07,
      "loss": 0.6164,
      "step": 821
    },
    {
      "epoch": 0.008303030303030303,
      "grad_norm": 0.2573327422142029,
      "learning_rate": 5.535353535353536e-07,
      "loss": 0.5572,
      "step": 822
    },
    {
      "epoch": 0.008313131313131314,
      "grad_norm": 0.1351100653409958,
      "learning_rate": 5.542087542087543e-07,
      "loss": 0.6895,
      "step": 823
    },
    {
      "epoch": 0.008323232323232323,
      "grad_norm": 0.11733731627464294,
      "learning_rate": 5.54882154882155e-07,
      "loss": 0.5206,
      "step": 824
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 0.11618878692388535,
      "learning_rate": 5.555555555555555e-07,
      "loss": 0.7151,
      "step": 825
    },
    {
      "epoch": 0.008343434343434344,
      "grad_norm": 0.13704051077365875,
      "learning_rate": 5.562289562289563e-07,
      "loss": 0.4634,
      "step": 826
    },
    {
      "epoch": 0.008353535353535354,
      "grad_norm": 0.1106017604470253,
      "learning_rate": 5.56902356902357e-07,
      "loss": 0.6358,
      "step": 827
    },
    {
      "epoch": 0.008363636363636363,
      "grad_norm": 0.3407534658908844,
      "learning_rate": 5.575757575757576e-07,
      "loss": 0.9384,
      "step": 828
    },
    {
      "epoch": 0.008373737373737374,
      "grad_norm": 0.17754864692687988,
      "learning_rate": 5.582491582491583e-07,
      "loss": 1.026,
      "step": 829
    },
    {
      "epoch": 0.008383838383838384,
      "grad_norm": 0.11789155751466751,
      "learning_rate": 5.589225589225589e-07,
      "loss": 0.8071,
      "step": 830
    },
    {
      "epoch": 0.008393939393939395,
      "grad_norm": 0.2438424974679947,
      "learning_rate": 5.595959595959596e-07,
      "loss": 0.8435,
      "step": 831
    },
    {
      "epoch": 0.008404040404040403,
      "grad_norm": 0.16143575310707092,
      "learning_rate": 5.602693602693604e-07,
      "loss": 0.5294,
      "step": 832
    },
    {
      "epoch": 0.008414141414141414,
      "grad_norm": 0.20935530960559845,
      "learning_rate": 5.60942760942761e-07,
      "loss": 0.509,
      "step": 833
    },
    {
      "epoch": 0.008424242424242424,
      "grad_norm": 0.09967628121376038,
      "learning_rate": 5.616161616161616e-07,
      "loss": 0.6025,
      "step": 834
    },
    {
      "epoch": 0.008434343434343435,
      "grad_norm": 0.11452720314264297,
      "learning_rate": 5.622895622895623e-07,
      "loss": 0.554,
      "step": 835
    },
    {
      "epoch": 0.008444444444444444,
      "grad_norm": 0.14806361496448517,
      "learning_rate": 5.62962962962963e-07,
      "loss": 0.4321,
      "step": 836
    },
    {
      "epoch": 0.008454545454545454,
      "grad_norm": 0.18348681926727295,
      "learning_rate": 5.636363636363638e-07,
      "loss": 0.6636,
      "step": 837
    },
    {
      "epoch": 0.008464646464646465,
      "grad_norm": 0.24508912861347198,
      "learning_rate": 5.643097643097643e-07,
      "loss": 0.5954,
      "step": 838
    },
    {
      "epoch": 0.008474747474747475,
      "grad_norm": 0.1745966374874115,
      "learning_rate": 5.64983164983165e-07,
      "loss": 0.3924,
      "step": 839
    },
    {
      "epoch": 0.008484848484848486,
      "grad_norm": 0.17728452384471893,
      "learning_rate": 5.656565656565658e-07,
      "loss": 0.4771,
      "step": 840
    },
    {
      "epoch": 0.008494949494949494,
      "grad_norm": 0.13257743418216705,
      "learning_rate": 5.663299663299664e-07,
      "loss": 0.5362,
      "step": 841
    },
    {
      "epoch": 0.008505050505050505,
      "grad_norm": 0.11903123557567596,
      "learning_rate": 5.67003367003367e-07,
      "loss": 0.5557,
      "step": 842
    },
    {
      "epoch": 0.008515151515151516,
      "grad_norm": 0.2657492458820343,
      "learning_rate": 5.676767676767677e-07,
      "loss": 0.523,
      "step": 843
    },
    {
      "epoch": 0.008525252525252526,
      "grad_norm": 0.16824208199977875,
      "learning_rate": 5.683501683501684e-07,
      "loss": 0.4242,
      "step": 844
    },
    {
      "epoch": 0.008535353535353535,
      "grad_norm": 0.14853554964065552,
      "learning_rate": 5.690235690235691e-07,
      "loss": 0.8328,
      "step": 845
    },
    {
      "epoch": 0.008545454545454545,
      "grad_norm": 0.1262734979391098,
      "learning_rate": 5.696969696969698e-07,
      "loss": 0.5303,
      "step": 846
    },
    {
      "epoch": 0.008555555555555556,
      "grad_norm": 0.19675783812999725,
      "learning_rate": 5.703703703703704e-07,
      "loss": 0.8829,
      "step": 847
    },
    {
      "epoch": 0.008565656565656566,
      "grad_norm": 0.15554694831371307,
      "learning_rate": 5.71043771043771e-07,
      "loss": 0.6576,
      "step": 848
    },
    {
      "epoch": 0.008575757575757575,
      "grad_norm": 0.16256757080554962,
      "learning_rate": 5.717171717171718e-07,
      "loss": 0.6216,
      "step": 849
    },
    {
      "epoch": 0.008585858585858586,
      "grad_norm": 0.196786031126976,
      "learning_rate": 5.723905723905725e-07,
      "loss": 0.5604,
      "step": 850
    },
    {
      "epoch": 0.008595959595959596,
      "grad_norm": 0.140664204955101,
      "learning_rate": 5.730639730639732e-07,
      "loss": 0.6975,
      "step": 851
    },
    {
      "epoch": 0.008606060606060607,
      "grad_norm": 0.11824977397918701,
      "learning_rate": 5.737373737373738e-07,
      "loss": 0.2652,
      "step": 852
    },
    {
      "epoch": 0.008616161616161615,
      "grad_norm": 0.1544460654258728,
      "learning_rate": 5.744107744107744e-07,
      "loss": 0.5819,
      "step": 853
    },
    {
      "epoch": 0.008626262626262626,
      "grad_norm": 0.1903165876865387,
      "learning_rate": 5.750841750841752e-07,
      "loss": 0.3775,
      "step": 854
    },
    {
      "epoch": 0.008636363636363636,
      "grad_norm": 0.12256573140621185,
      "learning_rate": 5.757575757575758e-07,
      "loss": 0.9268,
      "step": 855
    },
    {
      "epoch": 0.008646464646464647,
      "grad_norm": 0.16516993939876556,
      "learning_rate": 5.764309764309764e-07,
      "loss": 0.9807,
      "step": 856
    },
    {
      "epoch": 0.008656565656565656,
      "grad_norm": 0.17704246938228607,
      "learning_rate": 5.771043771043772e-07,
      "loss": 0.6478,
      "step": 857
    },
    {
      "epoch": 0.008666666666666666,
      "grad_norm": 0.1727200448513031,
      "learning_rate": 5.777777777777778e-07,
      "loss": 0.5372,
      "step": 858
    },
    {
      "epoch": 0.008676767676767677,
      "grad_norm": 0.21961359679698944,
      "learning_rate": 5.784511784511785e-07,
      "loss": 0.5348,
      "step": 859
    },
    {
      "epoch": 0.008686868686868687,
      "grad_norm": 0.12317351996898651,
      "learning_rate": 5.791245791245792e-07,
      "loss": 0.7695,
      "step": 860
    },
    {
      "epoch": 0.008696969696969698,
      "grad_norm": 0.20298291742801666,
      "learning_rate": 5.797979797979798e-07,
      "loss": 0.7705,
      "step": 861
    },
    {
      "epoch": 0.008707070707070707,
      "grad_norm": 0.15523797273635864,
      "learning_rate": 5.804713804713805e-07,
      "loss": 0.5417,
      "step": 862
    },
    {
      "epoch": 0.008717171717171717,
      "grad_norm": 0.12090657651424408,
      "learning_rate": 5.811447811447812e-07,
      "loss": 1.2608,
      "step": 863
    },
    {
      "epoch": 0.008727272727272728,
      "grad_norm": 0.14855632185935974,
      "learning_rate": 5.818181818181819e-07,
      "loss": 0.5764,
      "step": 864
    },
    {
      "epoch": 0.008737373737373738,
      "grad_norm": 0.15951471030712128,
      "learning_rate": 5.824915824915826e-07,
      "loss": 0.4758,
      "step": 865
    },
    {
      "epoch": 0.008747474747474747,
      "grad_norm": 0.1276477575302124,
      "learning_rate": 5.831649831649832e-07,
      "loss": 0.8424,
      "step": 866
    },
    {
      "epoch": 0.008757575757575757,
      "grad_norm": 0.1115812286734581,
      "learning_rate": 5.838383838383839e-07,
      "loss": 0.7205,
      "step": 867
    },
    {
      "epoch": 0.008767676767676768,
      "grad_norm": 0.1557028591632843,
      "learning_rate": 5.845117845117846e-07,
      "loss": 0.6194,
      "step": 868
    },
    {
      "epoch": 0.008777777777777778,
      "grad_norm": 0.08648797869682312,
      "learning_rate": 5.851851851851852e-07,
      "loss": 0.4951,
      "step": 869
    },
    {
      "epoch": 0.008787878787878787,
      "grad_norm": 0.19664371013641357,
      "learning_rate": 5.858585858585859e-07,
      "loss": 0.6532,
      "step": 870
    },
    {
      "epoch": 0.008797979797979798,
      "grad_norm": 0.12008091807365417,
      "learning_rate": 5.865319865319866e-07,
      "loss": 0.5723,
      "step": 871
    },
    {
      "epoch": 0.008808080808080808,
      "grad_norm": 0.42745688557624817,
      "learning_rate": 5.872053872053873e-07,
      "loss": 0.7591,
      "step": 872
    },
    {
      "epoch": 0.008818181818181819,
      "grad_norm": 0.10813293606042862,
      "learning_rate": 5.878787878787879e-07,
      "loss": 0.4526,
      "step": 873
    },
    {
      "epoch": 0.008828282828282828,
      "grad_norm": 0.23841416835784912,
      "learning_rate": 5.885521885521886e-07,
      "loss": 1.1624,
      "step": 874
    },
    {
      "epoch": 0.008838383838383838,
      "grad_norm": 0.12975561618804932,
      "learning_rate": 5.892255892255893e-07,
      "loss": 0.3128,
      "step": 875
    },
    {
      "epoch": 0.008848484848484849,
      "grad_norm": 0.1433764398097992,
      "learning_rate": 5.898989898989899e-07,
      "loss": 0.6953,
      "step": 876
    },
    {
      "epoch": 0.008858585858585859,
      "grad_norm": 0.08838097006082535,
      "learning_rate": 5.905723905723907e-07,
      "loss": 0.5511,
      "step": 877
    },
    {
      "epoch": 0.008868686868686868,
      "grad_norm": 0.13946367800235748,
      "learning_rate": 5.912457912457913e-07,
      "loss": 0.585,
      "step": 878
    },
    {
      "epoch": 0.008878787878787878,
      "grad_norm": 0.10797251760959625,
      "learning_rate": 5.91919191919192e-07,
      "loss": 0.5359,
      "step": 879
    },
    {
      "epoch": 0.008888888888888889,
      "grad_norm": 0.14933177828788757,
      "learning_rate": 5.925925925925927e-07,
      "loss": 0.3845,
      "step": 880
    },
    {
      "epoch": 0.0088989898989899,
      "grad_norm": 0.1947561353445053,
      "learning_rate": 5.932659932659933e-07,
      "loss": 0.5239,
      "step": 881
    },
    {
      "epoch": 0.00890909090909091,
      "grad_norm": 0.15944606065750122,
      "learning_rate": 5.93939393939394e-07,
      "loss": 0.3615,
      "step": 882
    },
    {
      "epoch": 0.008919191919191919,
      "grad_norm": 0.13576358556747437,
      "learning_rate": 5.946127946127946e-07,
      "loss": 0.5795,
      "step": 883
    },
    {
      "epoch": 0.00892929292929293,
      "grad_norm": 0.12267760187387466,
      "learning_rate": 5.952861952861953e-07,
      "loss": 0.703,
      "step": 884
    },
    {
      "epoch": 0.00893939393939394,
      "grad_norm": 0.17584680020809174,
      "learning_rate": 5.959595959595961e-07,
      "loss": 0.8038,
      "step": 885
    },
    {
      "epoch": 0.00894949494949495,
      "grad_norm": 0.15711991488933563,
      "learning_rate": 5.966329966329967e-07,
      "loss": 0.4808,
      "step": 886
    },
    {
      "epoch": 0.008959595959595959,
      "grad_norm": 0.22643513977527618,
      "learning_rate": 5.973063973063973e-07,
      "loss": 0.5857,
      "step": 887
    },
    {
      "epoch": 0.00896969696969697,
      "grad_norm": 0.11711063981056213,
      "learning_rate": 5.979797979797981e-07,
      "loss": 0.4337,
      "step": 888
    },
    {
      "epoch": 0.00897979797979798,
      "grad_norm": 0.10109319537878036,
      "learning_rate": 5.986531986531987e-07,
      "loss": 0.5857,
      "step": 889
    },
    {
      "epoch": 0.00898989898989899,
      "grad_norm": 0.09480670094490051,
      "learning_rate": 5.993265993265994e-07,
      "loss": 0.5356,
      "step": 890
    },
    {
      "epoch": 0.009,
      "grad_norm": 0.12180300056934357,
      "learning_rate": 6.000000000000001e-07,
      "loss": 0.679,
      "step": 891
    },
    {
      "epoch": 0.00901010101010101,
      "grad_norm": 0.16831354796886444,
      "learning_rate": 6.006734006734007e-07,
      "loss": 0.5177,
      "step": 892
    },
    {
      "epoch": 0.00902020202020202,
      "grad_norm": 0.39484673738479614,
      "learning_rate": 6.013468013468015e-07,
      "loss": 1.0543,
      "step": 893
    },
    {
      "epoch": 0.00903030303030303,
      "grad_norm": 0.5034874081611633,
      "learning_rate": 6.020202020202021e-07,
      "loss": 0.6592,
      "step": 894
    },
    {
      "epoch": 0.00904040404040404,
      "grad_norm": 0.10884293168783188,
      "learning_rate": 6.026936026936027e-07,
      "loss": 0.5829,
      "step": 895
    },
    {
      "epoch": 0.00905050505050505,
      "grad_norm": 0.24882572889328003,
      "learning_rate": 6.033670033670034e-07,
      "loss": 0.5347,
      "step": 896
    },
    {
      "epoch": 0.00906060606060606,
      "grad_norm": 0.13705845177173615,
      "learning_rate": 6.040404040404041e-07,
      "loss": 0.7387,
      "step": 897
    },
    {
      "epoch": 0.009070707070707071,
      "grad_norm": 0.12954823672771454,
      "learning_rate": 6.047138047138048e-07,
      "loss": 0.9595,
      "step": 898
    },
    {
      "epoch": 0.00908080808080808,
      "grad_norm": 0.10161365568637848,
      "learning_rate": 6.053872053872055e-07,
      "loss": 0.7629,
      "step": 899
    },
    {
      "epoch": 0.00909090909090909,
      "grad_norm": 0.205815389752388,
      "learning_rate": 6.060606060606061e-07,
      "loss": 0.5873,
      "step": 900
    },
    {
      "epoch": 0.009101010101010101,
      "grad_norm": 0.18232400715351105,
      "learning_rate": 6.067340067340067e-07,
      "loss": 0.9705,
      "step": 901
    },
    {
      "epoch": 0.009111111111111111,
      "grad_norm": 0.10841051489114761,
      "learning_rate": 6.074074074074075e-07,
      "loss": 0.5598,
      "step": 902
    },
    {
      "epoch": 0.009121212121212122,
      "grad_norm": 0.21568433940410614,
      "learning_rate": 6.080808080808082e-07,
      "loss": 1.0228,
      "step": 903
    },
    {
      "epoch": 0.00913131313131313,
      "grad_norm": 0.128280907869339,
      "learning_rate": 6.087542087542088e-07,
      "loss": 0.5377,
      "step": 904
    },
    {
      "epoch": 0.009141414141414141,
      "grad_norm": 0.1946801245212555,
      "learning_rate": 6.094276094276095e-07,
      "loss": 0.7374,
      "step": 905
    },
    {
      "epoch": 0.009151515151515152,
      "grad_norm": 0.13811226189136505,
      "learning_rate": 6.101010101010101e-07,
      "loss": 0.8677,
      "step": 906
    },
    {
      "epoch": 0.009161616161616162,
      "grad_norm": 0.21861086785793304,
      "learning_rate": 6.107744107744108e-07,
      "loss": 0.5598,
      "step": 907
    },
    {
      "epoch": 0.009171717171717171,
      "grad_norm": 0.15395434200763702,
      "learning_rate": 6.114478114478115e-07,
      "loss": 0.4152,
      "step": 908
    },
    {
      "epoch": 0.009181818181818182,
      "grad_norm": 0.10857347398996353,
      "learning_rate": 6.121212121212121e-07,
      "loss": 0.4461,
      "step": 909
    },
    {
      "epoch": 0.009191919191919192,
      "grad_norm": 0.20702604949474335,
      "learning_rate": 6.127946127946129e-07,
      "loss": 0.9702,
      "step": 910
    },
    {
      "epoch": 0.009202020202020203,
      "grad_norm": 0.12220042943954468,
      "learning_rate": 6.134680134680135e-07,
      "loss": 0.7483,
      "step": 911
    },
    {
      "epoch": 0.009212121212121211,
      "grad_norm": 0.10573285073041916,
      "learning_rate": 6.141414141414142e-07,
      "loss": 0.444,
      "step": 912
    },
    {
      "epoch": 0.009222222222222222,
      "grad_norm": 0.22967691719532013,
      "learning_rate": 6.148148148148149e-07,
      "loss": 0.5569,
      "step": 913
    },
    {
      "epoch": 0.009232323232323232,
      "grad_norm": 0.10626694560050964,
      "learning_rate": 6.154882154882155e-07,
      "loss": 0.5454,
      "step": 914
    },
    {
      "epoch": 0.009242424242424243,
      "grad_norm": 0.2000351846218109,
      "learning_rate": 6.161616161616162e-07,
      "loss": 0.8103,
      "step": 915
    },
    {
      "epoch": 0.009252525252525252,
      "grad_norm": 0.11848441511392593,
      "learning_rate": 6.168350168350169e-07,
      "loss": 0.6397,
      "step": 916
    },
    {
      "epoch": 0.009262626262626262,
      "grad_norm": 0.2500256299972534,
      "learning_rate": 6.175084175084176e-07,
      "loss": 1.1228,
      "step": 917
    },
    {
      "epoch": 0.009272727272727273,
      "grad_norm": 0.20684920251369476,
      "learning_rate": 6.181818181818182e-07,
      "loss": 0.6691,
      "step": 918
    },
    {
      "epoch": 0.009282828282828283,
      "grad_norm": 0.14282579720020294,
      "learning_rate": 6.188552188552189e-07,
      "loss": 0.9794,
      "step": 919
    },
    {
      "epoch": 0.009292929292929294,
      "grad_norm": 0.13062605261802673,
      "learning_rate": 6.195286195286196e-07,
      "loss": 0.7315,
      "step": 920
    },
    {
      "epoch": 0.009303030303030303,
      "grad_norm": 0.10070574283599854,
      "learning_rate": 6.202020202020202e-07,
      "loss": 0.3491,
      "step": 921
    },
    {
      "epoch": 0.009313131313131313,
      "grad_norm": 0.1501857191324234,
      "learning_rate": 6.208754208754209e-07,
      "loss": 0.5343,
      "step": 922
    },
    {
      "epoch": 0.009323232323232324,
      "grad_norm": 0.1677149534225464,
      "learning_rate": 6.215488215488216e-07,
      "loss": 0.4981,
      "step": 923
    },
    {
      "epoch": 0.009333333333333334,
      "grad_norm": 0.116569384932518,
      "learning_rate": 6.222222222222223e-07,
      "loss": 0.6011,
      "step": 924
    },
    {
      "epoch": 0.009343434343434343,
      "grad_norm": 0.14707882702350616,
      "learning_rate": 6.22895622895623e-07,
      "loss": 0.5411,
      "step": 925
    },
    {
      "epoch": 0.009353535353535353,
      "grad_norm": 0.14227581024169922,
      "learning_rate": 6.235690235690236e-07,
      "loss": 0.7189,
      "step": 926
    },
    {
      "epoch": 0.009363636363636364,
      "grad_norm": 0.09600906074047089,
      "learning_rate": 6.242424242424243e-07,
      "loss": 0.691,
      "step": 927
    },
    {
      "epoch": 0.009373737373737374,
      "grad_norm": 0.26775214076042175,
      "learning_rate": 6.24915824915825e-07,
      "loss": 0.5494,
      "step": 928
    },
    {
      "epoch": 0.009383838383838383,
      "grad_norm": 0.10540413111448288,
      "learning_rate": 6.255892255892257e-07,
      "loss": 0.4725,
      "step": 929
    },
    {
      "epoch": 0.009393939393939394,
      "grad_norm": 0.18460296094417572,
      "learning_rate": 6.262626262626264e-07,
      "loss": 0.8507,
      "step": 930
    },
    {
      "epoch": 0.009404040404040404,
      "grad_norm": 0.15481285750865936,
      "learning_rate": 6.26936026936027e-07,
      "loss": 0.7586,
      "step": 931
    },
    {
      "epoch": 0.009414141414141415,
      "grad_norm": 0.256612092256546,
      "learning_rate": 6.276094276094277e-07,
      "loss": 0.6441,
      "step": 932
    },
    {
      "epoch": 0.009424242424242424,
      "grad_norm": 0.13620667159557343,
      "learning_rate": 6.282828282828284e-07,
      "loss": 0.5899,
      "step": 933
    },
    {
      "epoch": 0.009434343434343434,
      "grad_norm": 0.10793551057577133,
      "learning_rate": 6.28956228956229e-07,
      "loss": 0.5543,
      "step": 934
    },
    {
      "epoch": 0.009444444444444445,
      "grad_norm": 0.1624981164932251,
      "learning_rate": 6.296296296296296e-07,
      "loss": 0.6382,
      "step": 935
    },
    {
      "epoch": 0.009454545454545455,
      "grad_norm": 0.15050460398197174,
      "learning_rate": 6.303030303030304e-07,
      "loss": 0.6883,
      "step": 936
    },
    {
      "epoch": 0.009464646464646464,
      "grad_norm": 0.11471404135227203,
      "learning_rate": 6.30976430976431e-07,
      "loss": 0.7208,
      "step": 937
    },
    {
      "epoch": 0.009474747474747474,
      "grad_norm": 0.16455203294754028,
      "learning_rate": 6.316498316498317e-07,
      "loss": 0.8007,
      "step": 938
    },
    {
      "epoch": 0.009484848484848485,
      "grad_norm": 0.13860420882701874,
      "learning_rate": 6.323232323232324e-07,
      "loss": 0.9911,
      "step": 939
    },
    {
      "epoch": 0.009494949494949495,
      "grad_norm": 0.1549687385559082,
      "learning_rate": 6.32996632996633e-07,
      "loss": 0.7877,
      "step": 940
    },
    {
      "epoch": 0.009505050505050506,
      "grad_norm": 0.15368852019309998,
      "learning_rate": 6.336700336700337e-07,
      "loss": 0.3497,
      "step": 941
    },
    {
      "epoch": 0.009515151515151515,
      "grad_norm": 0.1333560347557068,
      "learning_rate": 6.343434343434345e-07,
      "loss": 0.9319,
      "step": 942
    },
    {
      "epoch": 0.009525252525252525,
      "grad_norm": 0.16411380469799042,
      "learning_rate": 6.350168350168351e-07,
      "loss": 0.8035,
      "step": 943
    },
    {
      "epoch": 0.009535353535353536,
      "grad_norm": 0.202254518866539,
      "learning_rate": 6.356902356902358e-07,
      "loss": 0.4734,
      "step": 944
    },
    {
      "epoch": 0.009545454545454546,
      "grad_norm": 0.11579407751560211,
      "learning_rate": 6.363636363636364e-07,
      "loss": 0.5772,
      "step": 945
    },
    {
      "epoch": 0.009555555555555555,
      "grad_norm": 0.23345138132572174,
      "learning_rate": 6.370370370370371e-07,
      "loss": 0.8181,
      "step": 946
    },
    {
      "epoch": 0.009565656565656565,
      "grad_norm": 0.13005051016807556,
      "learning_rate": 6.377104377104378e-07,
      "loss": 0.4456,
      "step": 947
    },
    {
      "epoch": 0.009575757575757576,
      "grad_norm": 0.32789161801338196,
      "learning_rate": 6.383838383838384e-07,
      "loss": 0.7598,
      "step": 948
    },
    {
      "epoch": 0.009585858585858587,
      "grad_norm": 0.11120095103979111,
      "learning_rate": 6.390572390572391e-07,
      "loss": 0.5896,
      "step": 949
    },
    {
      "epoch": 0.009595959595959595,
      "grad_norm": 0.30417320132255554,
      "learning_rate": 6.397306397306398e-07,
      "loss": 0.5197,
      "step": 950
    },
    {
      "epoch": 0.009606060606060606,
      "grad_norm": 0.1691981703042984,
      "learning_rate": 6.404040404040404e-07,
      "loss": 0.5324,
      "step": 951
    },
    {
      "epoch": 0.009616161616161616,
      "grad_norm": 0.1495608538389206,
      "learning_rate": 6.410774410774412e-07,
      "loss": 1.0495,
      "step": 952
    },
    {
      "epoch": 0.009626262626262627,
      "grad_norm": 0.1839503049850464,
      "learning_rate": 6.417508417508418e-07,
      "loss": 0.3632,
      "step": 953
    },
    {
      "epoch": 0.009636363636363636,
      "grad_norm": 0.17879875004291534,
      "learning_rate": 6.424242424242424e-07,
      "loss": 0.6203,
      "step": 954
    },
    {
      "epoch": 0.009646464646464646,
      "grad_norm": 0.09108888357877731,
      "learning_rate": 6.430976430976431e-07,
      "loss": 0.4589,
      "step": 955
    },
    {
      "epoch": 0.009656565656565657,
      "grad_norm": 0.20170840620994568,
      "learning_rate": 6.437710437710439e-07,
      "loss": 0.3931,
      "step": 956
    },
    {
      "epoch": 0.009666666666666667,
      "grad_norm": 0.21640443801879883,
      "learning_rate": 6.444444444444445e-07,
      "loss": 1.0154,
      "step": 957
    },
    {
      "epoch": 0.009676767676767676,
      "grad_norm": 0.207742840051651,
      "learning_rate": 6.451178451178452e-07,
      "loss": 0.5091,
      "step": 958
    },
    {
      "epoch": 0.009686868686868686,
      "grad_norm": 0.17010799050331116,
      "learning_rate": 6.457912457912459e-07,
      "loss": 0.6402,
      "step": 959
    },
    {
      "epoch": 0.009696969696969697,
      "grad_norm": 0.24875393509864807,
      "learning_rate": 6.464646464646465e-07,
      "loss": 0.5141,
      "step": 960
    },
    {
      "epoch": 0.009707070707070707,
      "grad_norm": 0.23490317165851593,
      "learning_rate": 6.471380471380472e-07,
      "loss": 0.9586,
      "step": 961
    },
    {
      "epoch": 0.009717171717171718,
      "grad_norm": 0.1784280240535736,
      "learning_rate": 6.478114478114479e-07,
      "loss": 0.2972,
      "step": 962
    },
    {
      "epoch": 0.009727272727272727,
      "grad_norm": 0.1972924917936325,
      "learning_rate": 6.484848484848485e-07,
      "loss": 0.765,
      "step": 963
    },
    {
      "epoch": 0.009737373737373737,
      "grad_norm": 0.3511125147342682,
      "learning_rate": 6.491582491582492e-07,
      "loss": 0.8399,
      "step": 964
    },
    {
      "epoch": 0.009747474747474748,
      "grad_norm": 0.2269652932882309,
      "learning_rate": 6.498316498316499e-07,
      "loss": 1.1824,
      "step": 965
    },
    {
      "epoch": 0.009757575757575758,
      "grad_norm": 0.11786013096570969,
      "learning_rate": 6.505050505050506e-07,
      "loss": 0.7551,
      "step": 966
    },
    {
      "epoch": 0.009767676767676767,
      "grad_norm": 0.3300611972808838,
      "learning_rate": 6.511784511784512e-07,
      "loss": 0.8396,
      "step": 967
    },
    {
      "epoch": 0.009777777777777778,
      "grad_norm": 0.10145890712738037,
      "learning_rate": 6.518518518518518e-07,
      "loss": 0.6659,
      "step": 968
    },
    {
      "epoch": 0.009787878787878788,
      "grad_norm": 0.10671713948249817,
      "learning_rate": 6.525252525252527e-07,
      "loss": 0.708,
      "step": 969
    },
    {
      "epoch": 0.009797979797979799,
      "grad_norm": 0.2364225834608078,
      "learning_rate": 6.531986531986533e-07,
      "loss": 0.4211,
      "step": 970
    },
    {
      "epoch": 0.009808080808080807,
      "grad_norm": 0.18957865238189697,
      "learning_rate": 6.538720538720539e-07,
      "loss": 0.7578,
      "step": 971
    },
    {
      "epoch": 0.009818181818181818,
      "grad_norm": 0.1859760880470276,
      "learning_rate": 6.545454545454547e-07,
      "loss": 0.6248,
      "step": 972
    },
    {
      "epoch": 0.009828282828282828,
      "grad_norm": 0.17212870717048645,
      "learning_rate": 6.552188552188553e-07,
      "loss": 0.9324,
      "step": 973
    },
    {
      "epoch": 0.009838383838383839,
      "grad_norm": 0.10168004035949707,
      "learning_rate": 6.558922558922559e-07,
      "loss": 0.6097,
      "step": 974
    },
    {
      "epoch": 0.009848484848484848,
      "grad_norm": 0.14271311461925507,
      "learning_rate": 6.565656565656567e-07,
      "loss": 0.7053,
      "step": 975
    },
    {
      "epoch": 0.009858585858585858,
      "grad_norm": 0.2653055787086487,
      "learning_rate": 6.572390572390573e-07,
      "loss": 0.5745,
      "step": 976
    },
    {
      "epoch": 0.009868686868686869,
      "grad_norm": 0.4062998592853546,
      "learning_rate": 6.579124579124579e-07,
      "loss": 1.0494,
      "step": 977
    },
    {
      "epoch": 0.00987878787878788,
      "grad_norm": 0.22705109417438507,
      "learning_rate": 6.585858585858586e-07,
      "loss": 0.3442,
      "step": 978
    },
    {
      "epoch": 0.009888888888888888,
      "grad_norm": 0.10847944021224976,
      "learning_rate": 6.592592592592593e-07,
      "loss": 0.5387,
      "step": 979
    },
    {
      "epoch": 0.009898989898989899,
      "grad_norm": 0.21367155015468597,
      "learning_rate": 6.5993265993266e-07,
      "loss": 0.7438,
      "step": 980
    },
    {
      "epoch": 0.009909090909090909,
      "grad_norm": 0.184495210647583,
      "learning_rate": 6.606060606060606e-07,
      "loss": 0.5443,
      "step": 981
    },
    {
      "epoch": 0.00991919191919192,
      "grad_norm": 0.24014772474765778,
      "learning_rate": 6.612794612794614e-07,
      "loss": 0.8034,
      "step": 982
    },
    {
      "epoch": 0.00992929292929293,
      "grad_norm": 0.17587421834468842,
      "learning_rate": 6.619528619528621e-07,
      "loss": 0.6824,
      "step": 983
    },
    {
      "epoch": 0.009939393939393939,
      "grad_norm": 0.1103702187538147,
      "learning_rate": 6.626262626262627e-07,
      "loss": 0.7631,
      "step": 984
    },
    {
      "epoch": 0.00994949494949495,
      "grad_norm": 0.1235569566488266,
      "learning_rate": 6.632996632996634e-07,
      "loss": 0.5732,
      "step": 985
    },
    {
      "epoch": 0.00995959595959596,
      "grad_norm": 0.17740149796009064,
      "learning_rate": 6.639730639730641e-07,
      "loss": 0.7973,
      "step": 986
    },
    {
      "epoch": 0.00996969696969697,
      "grad_norm": 0.10783466696739197,
      "learning_rate": 6.646464646464647e-07,
      "loss": 0.6434,
      "step": 987
    },
    {
      "epoch": 0.00997979797979798,
      "grad_norm": 0.10532189905643463,
      "learning_rate": 6.653198653198653e-07,
      "loss": 0.7583,
      "step": 988
    },
    {
      "epoch": 0.00998989898989899,
      "grad_norm": 0.24685820937156677,
      "learning_rate": 6.659932659932661e-07,
      "loss": 0.6749,
      "step": 989
    },
    {
      "epoch": 0.01,
      "grad_norm": 0.23153826594352722,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.6521,
      "step": 990
    },
    {
      "epoch": 0.01001010101010101,
      "grad_norm": 0.15894705057144165,
      "learning_rate": 6.673400673400673e-07,
      "loss": 0.4596,
      "step": 991
    },
    {
      "epoch": 0.01002020202020202,
      "grad_norm": 0.20597432553768158,
      "learning_rate": 6.680134680134681e-07,
      "loss": 0.7759,
      "step": 992
    },
    {
      "epoch": 0.01003030303030303,
      "grad_norm": 0.17528441548347473,
      "learning_rate": 6.686868686868687e-07,
      "loss": 0.933,
      "step": 993
    },
    {
      "epoch": 0.01004040404040404,
      "grad_norm": 0.10763565450906754,
      "learning_rate": 6.693602693602693e-07,
      "loss": 0.5663,
      "step": 994
    },
    {
      "epoch": 0.010050505050505051,
      "grad_norm": 0.22885887324810028,
      "learning_rate": 6.700336700336702e-07,
      "loss": 1.1202,
      "step": 995
    },
    {
      "epoch": 0.01006060606060606,
      "grad_norm": 0.12738904356956482,
      "learning_rate": 6.707070707070708e-07,
      "loss": 0.7415,
      "step": 996
    },
    {
      "epoch": 0.01007070707070707,
      "grad_norm": 0.22539325058460236,
      "learning_rate": 6.713804713804715e-07,
      "loss": 1.2173,
      "step": 997
    },
    {
      "epoch": 0.01008080808080808,
      "grad_norm": 0.1183452233672142,
      "learning_rate": 6.720538720538721e-07,
      "loss": 0.6068,
      "step": 998
    },
    {
      "epoch": 0.010090909090909091,
      "grad_norm": 0.23646919429302216,
      "learning_rate": 6.727272727272728e-07,
      "loss": 0.9122,
      "step": 999
    },
    {
      "epoch": 0.010101010101010102,
      "grad_norm": 0.18201960623264313,
      "learning_rate": 6.734006734006735e-07,
      "loss": 0.6236,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 297000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 348587108119296.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
